{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher-Student Training Script \n",
    "\n",
    "(In progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Visualization script for Go1 with height scanner.\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Tell XLA to use Triton GEMM\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "\n",
    "import mujoco\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from environments.custom_env import Joystick, default_config\n",
    "\n",
    "from datetime import datetime\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import functools\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import Image as IPyimage, display, HTML, clear_output\n",
    "\n",
    "from utils import render_video_during_training, evaluate_policy\n",
    "\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True   # Show visual geoms\n",
    "scene_option.geomgroup[3] = False  # Hide collision geoms\n",
    "scene_option.geomgroup[5] = True   # Show sites (including height scanner visualization)\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Show contact points\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_RANGEFINDER] = True\n",
    "print(\"Creating Visualization...\")\n",
    "\n",
    "xml_path = '../environments/custom_env.xml' # 'custom_env_debug_wall.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "\n",
    "# JIT compile the functions for speed\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_terrain_height = jax.jit(env._get_torso_terrain_height)\n",
    "\n",
    "seed = 1234\n",
    "num_envs = ()\n",
    "key = jax.random.PRNGKey(seed)\n",
    "key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "key_envs = jax.random.split(key_env, num_envs)\n",
    "env_state = jit_reset(key_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Policy\n",
    "\n",
    "- pretrained inside train.ipynb\n",
    "- we want to load the parameters\n",
    "\n",
    "- Inputs: privileged_state with heightmap\n",
    "- Output: action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters for pre-trained teacher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from brax.training.acme import running_statistics\n",
    "\n",
    "\n",
    "# Needs to match training\n",
    "obs_shape = (96,)\n",
    "action_size = env.action_size\n",
    "\n",
    "# Observation normalisation\n",
    "loaded_params = np.load(\"../parameters/params.npy\", allow_pickle=True)\n",
    "normalizer_params = loaded_params[0]  \n",
    "normalize = running_statistics.normalize\n",
    "\n",
    "# Setup\n",
    "ppo_params = locomotion_params.brax_ppo_config('Go1JoystickRoughTerrain')\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "if \"network_factory\" in ppo_params:\n",
    "    network_factory = functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    )\n",
    "\n",
    "ppo_network = network_factory(\n",
    "    obs_shape, action_size, preprocess_observations_fn=normalize\n",
    ")\n",
    "\n",
    "init_params = ppo_losses.PPONetworkParams(\n",
    "    policy=ppo_network.policy_network.init(key_policy),\n",
    "    value=ppo_network.value_network.init(key_value),\n",
    ")\n",
    "\n",
    "# Create policy function\n",
    "make_policy = ppo_networks.make_inference_fn(ppo_network)\n",
    "\n",
    "params = np.load(\"../parameters/params.npy\", allow_pickle=True)\n",
    "\n",
    "jit_inference_fn   = jax.jit(make_policy(params, deterministic=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- Teacher is run for one episode at a time with privileged observations\n",
    "- Non-priveleged observations are saved as student input\n",
    "- action distribution (logits) are saved as student targets\n",
    "- RNN is trained with data from episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.custom_ppo_train import _maybe_wrap_env\n",
    "\n",
    "seed = 42\n",
    "\n",
    "action_size = 12\n",
    "hidden_size = (256, 128, 64)\n",
    "# hidden_size = 64\n",
    "\n",
    "episodes = 1000 # preivously 1000\n",
    "envs_per_episode = 1\n",
    "episode_length = 1024 # preivously 1024\n",
    "action_repeat = 1\n",
    "\n",
    "learning_rate = 1e-4 # preivously 1e-5\n",
    "\n",
    "teacher_visualisation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Policy\n",
    "\n",
    "Experiment 1: training with a newly initialized LSTM for a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Definition\n",
    "from typing import Sequence\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "class StudentPolicy(nn.Module):\n",
    "    \"\"\"Feedforward student policy that mimics teacher's action distribution\"\"\"\n",
    "    action_size: int\n",
    "    hidden_size: Sequence[int]   # Number of units in the hidden layer\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # x shape: (batch_size, obs_dim)\n",
    "        \n",
    "        # # First dense layer with activation\n",
    "        # x = nn.Dense(self.hidden_size)(x)\n",
    "        # x = nn.relu(x)  # or use nn.tanh(x) if preferred\n",
    "        \n",
    "        for feat in self.hidden_size[:-1]:\n",
    "            x = nn.relu(nn.Dense(feat)(x))\n",
    "        x = nn.Dense(self.hidden_size[-1])(x)\n",
    "\n",
    "        # Second dense layer to output logits (actions + log_stds)\n",
    "        logits = nn.Dense(features=2 * self.action_size)(x)\n",
    "        # logits = nn.Dense(features=self.action_size)(x)\n",
    "        \n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize student network\n",
    "student_obs_dim = 52\n",
    "batch_size = 128\n",
    "episode_length = 1024\n",
    "batches = episode_length // batch_size\n",
    "\n",
    "# Create student network\n",
    "student_net = StudentPolicy(action_size = action_size, hidden_size = hidden_size)\n",
    "\n",
    "# Initialize with dummy data\n",
    "dummy_input = jnp.ones((batch_size, student_obs_dim))\n",
    "key_student = jax.random.PRNGKey(42)\n",
    "student_params = student_net.init(key_student, dummy_input)\n",
    "\n",
    "print(f\"Student network initialized!\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Expected output shape: (batch_size, 24) for logits\")\n",
    "\n",
    "# Test student network\n",
    "test_output = student_net.apply(student_params, dummy_input)\n",
    "print(f\"Test output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Teacher-Student Training\n",
    "Define the functions needed to extract logits from the teacher network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get logits from teacher network\n",
    "@jax.jit\n",
    "def get_teacher_logits(params, observations):\n",
    "    param_subset = (params[0], params[1])  # normalizer and policy params\n",
    "    return ppo_network.policy_network.apply(*param_subset, observations)\n",
    "\n",
    "# Training setup\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(student_params)\n",
    "\n",
    "# Training function\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, inputs, targets):\n",
    "    def loss_fn(params):\n",
    "        # predictions = student_net.apply(params, inputs)\n",
    "        # loss = jnp.mean((predictions - targets) ** 2)\n",
    "        # return loss\n",
    "\n",
    "        # Student prediction (logits)\n",
    "        student_logits = student_net.apply(params, input)\n",
    "\n",
    "        # Split logits into means and log_stds\n",
    "        student_means = student_logits[:, :action_size]\n",
    "        student_log_stds = student_logits[:, action_size:]\n",
    "\n",
    "        teacher_means = targets[:, :action_size]\n",
    "        teacher_log_stds = targets[:, action_size:]\n",
    "\n",
    "        # log to exp\n",
    "        student_stds = jnp.exp(student_log_stds)\n",
    "        teacher_stds = jnp.exp(teacher_log_stds)\n",
    "\n",
    "        # Compute KL div between two gaussian dist\n",
    "        kl_div = (\n",
    "            teacher_log_stds - student_log_stds +\n",
    "            (student_stds ** 2 + (student_means - teacher_means) ** 2) /\n",
    "            (2 * teacher_stds ** 2) - 0.5\n",
    "        )\n",
    "\n",
    "        # Sum over action dimensions and average over batch\n",
    "        loss = jnp.mean(jnp.sum(kl_div, axis=-1))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mujoco_playground._src.gait import draw_joystick_command\n",
    "env_cfg = default_config()\n",
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [0.0, 0.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2*jp.pi]\n",
    "\n",
    "\n",
    "# Loop over episodes\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "for episode in range(episodes):\n",
    "    print(f\"\\n Episode {episode + 1}/{episodes} \")\n",
    "    \n",
    "    key = jax.random.PRNGKey(seed + episode)\n",
    "    key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "\n",
    "    wrapper_env = _maybe_wrap_env(\n",
    "        env,\n",
    "        wrap_env=True,\n",
    "        num_envs=envs_per_episode,\n",
    "        episode_length=episode_length,\n",
    "        action_repeat=action_repeat,\n",
    "        key_env=key_env,\n",
    "    )\n",
    "\n",
    "    # Reset environment \n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key_envs = jax.random.split(key_env, num_envs)\n",
    "    env_state = reset_fn(key_envs)\n",
    "\n",
    "    rng = jax.random.PRNGKey(episode)\n",
    "    # raw_command = jax.random.uniform(rng, shape=(3), minval=0.0, maxval=1.0)\n",
    "    raw_command = jp.array([0.5, 0.0, 0.0]) \n",
    "\n",
    "    command = jp.array([\n",
    "        raw_command[0] * env_cfg.command_config.a[0], \n",
    "        raw_command[1] * env_cfg.command_config.a[1],\n",
    "        raw_command[2] * env_cfg.command_config.a[2] \n",
    "    ])\n",
    "    state = jit_reset(rng)\n",
    "    state.info[\"command\"] = command\n",
    "\n",
    "    # Visualisation storing\n",
    "    rollout = []\n",
    "    modify_scene_fns = []\n",
    "\n",
    "    # Training data storing\n",
    "    student_inputs = []\n",
    "    student_targets = []\n",
    "    \n",
    "    for step in range(episode_length):\n",
    "        # Get teacher action and logits\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        ctrl, actions = jit_inference_fn(state.obs, act_rng)\n",
    "        # print(f\"control after inference: {ctrl}\")\n",
    "\n",
    "        # Get teacher logits (distribution parameters)\n",
    "        param_subset = (params[0], params[1])\n",
    "        logits = ppo_network.policy_network.apply(*param_subset, state.obs)\n",
    "\n",
    "        # TO TEST IF CONVERSION WORKS\n",
    "        # Convert logits to action\n",
    "        # parametric_action_distribution = ppo_network.parametric_action_distribution\n",
    "        # raw_action = parametric_action_distribution.sample_no_postprocessing(logits, rng)\n",
    "        # ctrl = parametric_action_distribution.postprocess(raw_action)\n",
    "        # ctrl = ppo_network.parametric_action_distribution.mode(logits) # This seems to be the correct way to do it\n",
    "        # print(f\"control after conversion back and fourth: {ctrl}\")\n",
    "        \n",
    "        # Store data for student training\n",
    "        student_inputs.append(state.obs['state'])  # Non-privileged observations\n",
    "        student_targets.append(logits)  # Teacher's action distribution logits\n",
    "        # student_targets.append(ctrl)  # Teacher's action distribution ctrl\n",
    "        \n",
    "        # Step environment\n",
    "        state = jit_step(state, ctrl)\n",
    "        state.info[\"command\"] = command\n",
    "        \n",
    "        # Visualization data\n",
    "        rollout.append(state)\n",
    "        xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "        xyz += np.array([0, 0, 0.2])\n",
    "        x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "        yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "        modify_scene_fns.append(\n",
    "            functools.partial(\n",
    "                draw_joystick_command,\n",
    "                cmd=state.info[\"command\"],\n",
    "                xyz=xyz,\n",
    "                theta=yaw,\n",
    "                scl=abs(state.info[\"command\"][0]) / env_cfg.command_config.a[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Prepare training data\n",
    "    student_inputs_array = jnp.array(student_inputs)  # (1024, 52)\n",
    "    student_targets_array = jnp.array(student_targets)  # (1024, 24)\n",
    "        \n",
    "    # Train student\n",
    "    total_loss = 0\n",
    "    for batch_idx in range(batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_inputs = student_inputs_array[start_idx:end_idx]\n",
    "        batch_targets = student_targets_array[start_idx:end_idx]\n",
    "        \n",
    "        student_params, opt_state, loss = train_step(\n",
    "            student_params, opt_state, batch_inputs, batch_targets\n",
    "        )\n",
    "        total_loss += loss\n",
    "    training_losses.append(total_loss / batches)\n",
    "\n",
    "    # Optional visualization\n",
    "    if teacher_visualisation: \n",
    "        render_every = 2\n",
    "        fps = 1.0 / env.dt / render_every\n",
    "        traj = rollout[::render_every]\n",
    "        mod_fns = modify_scene_fns[::render_every]\n",
    "\n",
    "        scene_option = mujoco.MjvOption()\n",
    "        scene_option.geomgroup[2] = True\n",
    "        scene_option.geomgroup[3] = False\n",
    "        scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "        scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "        scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "        frames = env.render(\n",
    "            traj,\n",
    "            camera=\"track\",\n",
    "            scene_option=scene_option,\n",
    "            width=640,\n",
    "            height=480,\n",
    "            modify_scene_fns=mod_fns,\n",
    "        )   \n",
    "        media.show_video(frames, fps=fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(1, len(training_losses) + 1), training_losses)\n",
    "plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"MSE Loss\")\n",
    "plt.ylabel(\"KL Divergence Loss\")\n",
    "plt.legend((\"Training Loss\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save student RNN parameters\n",
    "import pickle\n",
    "\n",
    "# Save\n",
    "with open('student_params_MLP_MSE_1000episodes.pkl', 'wb') as f:\n",
    "    pickle.dump(student_params, f)\n",
    "print(\"Student saved to student_params_MLP_MSE_1000episodes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OPTIONAL): Load Trained RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load student RNN parameters\n",
    "with open('student_params_MLP_MSE_1000episodes.pkl', 'rb') as f:\n",
    "    student_params = pickle.load(f)\n",
    "print(\"Student loaded from student_params_MLP_MSE_1000episodes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Student Polcicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Environment Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env config\n",
    "student_env_cfg = default_config()\n",
    "student_env_cfg.pert_config.enable = True\n",
    "student_env_cfg.pert_config.velocity_kick = [0.0, 0.0]\n",
    "student_env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "student_env_cfg.command_config.a = [1.5, 0.8, 2*jp.pi] # Max command values\n",
    "\n",
    "seed = 42\n",
    "\n",
    "num_episodes = 5\n",
    "episode_length = 500 # previously 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student eval\n",
    "from mujoco_playground._src.gait import draw_joystick_command\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    key = jax.random.PRNGKey(episode + seed)\n",
    "    key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "\n",
    "    wrapper_env = _maybe_wrap_env(\n",
    "    env,\n",
    "    wrap_env = True,\n",
    "    num_envs = 1,\n",
    "    episode_length = episode_length,\n",
    "    action_repeat = 1,\n",
    "    key_env = key_env,\n",
    "    )\n",
    "\n",
    "    # Reset environment \n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key_envs = jax.random.split(key_env, num_envs)\n",
    "    env_state = reset_fn(key_envs)\n",
    "\n",
    "    \n",
    "    # Set commands \n",
    "    rng = jax.random.PRNGKey(episode)\n",
    "    # raw_command = jax.random.uniform(rng, shape=(3), minval=0.0, maxval=1.0)\n",
    "    raw_command = jp.array([0.5, 0.0, 0.0])  # Hard coded for testing\n",
    "\n",
    "    command = jp.array([\n",
    "        raw_command[0] * student_env_cfg.command_config.a[0], \n",
    "        raw_command[1] * student_env_cfg.command_config.a[1],\n",
    "        raw_command[2] * student_env_cfg.command_config.a[2] \n",
    "    ])\n",
    "    state = jit_reset(rng)\n",
    "    state.info[\"command\"] = command\n",
    "\n",
    "    # Visualisation storing\n",
    "    rollout = []\n",
    "    modify_scene_fns = []\n",
    "\n",
    "    for step in range(episode_length):\n",
    "        # feed non-priveleged observations to network\n",
    "        student_obs = state.obs['state'] \n",
    "\n",
    "        # Get student logits and convert to action\n",
    "        student_obs_batch = student_obs[:student_obs_dim].reshape(1, -1)\n",
    "        logits = student_net.apply(student_params, student_obs_batch).squeeze(0)\n",
    "        ctrl = ppo_network.parametric_action_distribution.mode(logits)\n",
    "\n",
    "        # ctrl = student_net.apply(student_params, student_obs_batch).squeeze(0)\n",
    "\n",
    "        # Take step\n",
    "        state = jit_step(state, ctrl)\n",
    "        state.info[\"command\"] = command\n",
    "\n",
    "        # Visualization magic\n",
    "        rollout.append(state)\n",
    "        xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "        xyz += np.array([0, 0, 0.2])\n",
    "        x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "        yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "        modify_scene_fns.append(\n",
    "            functools.partial(\n",
    "                draw_joystick_command,\n",
    "                cmd=state.info[\"command\"],\n",
    "                xyz=xyz,\n",
    "                theta=yaw,\n",
    "                scl=abs(state.info[\"command\"][0]) / student_env_cfg.command_config.a[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Display visualisation magic\n",
    "    render_every = 2\n",
    "    fps = 1.0 / env.dt / render_every\n",
    "    traj = rollout[::render_every]\n",
    "    mod_fns = modify_scene_fns[::render_every]\n",
    "\n",
    "    scene_option = mujoco.MjvOption()\n",
    "    scene_option.geomgroup[2] = True\n",
    "    scene_option.geomgroup[3] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "    frames = env.render(\n",
    "        traj,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=640,\n",
    "        height=480,\n",
    "        modify_scene_fns=mod_fns,\n",
    "    )   \n",
    "    media.show_video(frames, fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
