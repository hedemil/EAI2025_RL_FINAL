{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher-Student Training Script \n",
    "\n",
    "(In progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Visualization...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Visualization script for Go1 with height scanner.\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Add parent directory to path\n",
    "parent_dir = os.path.abspath('..')\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Tell XLA to use Triton GEMM\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "\n",
    "import mujoco\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from environments.custom_env import Joystick, default_config\n",
    "\n",
    "from datetime import datetime\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import functools\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import Image as IPyimage, display, HTML, clear_output\n",
    "\n",
    "from utils import render_video_during_training, evaluate_policy\n",
    "\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True   # Show visual geoms\n",
    "scene_option.geomgroup[3] = False  # Hide collision geoms\n",
    "scene_option.geomgroup[5] = True   # Show sites (including height scanner visualization)\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Show contact points\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_RANGEFINDER] = True\n",
    "print(\"Creating Visualization...\")\n",
    "\n",
    "xml_path = '../environments/custom_env.xml' # 'custom_env_debug_wall.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "\n",
    "# JIT compile the functions for speed\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_terrain_height = jax.jit(env._get_torso_terrain_height)\n",
    "\n",
    "seed = 1234\n",
    "# num_envs = ()\n",
    "# key = jax.random.PRNGKey(seed)\n",
    "# key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "# key_envs = jax.random.split(key_env, num_envs)\n",
    "# env_state = jit_reset(key_envs)\n",
    "\n",
    "num_envs = 1\n",
    "key = jax.random.PRNGKey(seed)\n",
    "key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "env_state = jit_reset(key_env)  # single key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Policy\n",
    "\n",
    "- pretrained inside train.ipynb\n",
    "- we want to load the parameters\n",
    "\n",
    "- Inputs: privileged_state with heightmap\n",
    "- Output: action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load parameters for pre-trained teacher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher param keys: ['normalizer', 'policy', 'value']\n",
      "Teacher policy loaded and compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.acme import running_statistics\n",
    "import functools\n",
    "import numpy as np\n",
    "import jax\n",
    "\n",
    "# --- Load saved teacher parameters ---\n",
    "loaded = np.load(\"../parameters/params.npy\", allow_pickle=True)\n",
    "\n",
    "# Case 1: Single serialized PyTree (most likely)\n",
    "if loaded.shape == ():  # scalar array containing a dict\n",
    "    params = loaded.item()\n",
    "# Case 2: Array of dicts (normalizer, policy, value)\n",
    "elif isinstance(loaded, np.ndarray) and loaded.dtype == object and len(loaded) == 3:\n",
    "    params = {\n",
    "        'normalizer': loaded[0],\n",
    "        'policy': loaded[1],\n",
    "        'value': loaded[2],\n",
    "    }\n",
    "else:\n",
    "    raise ValueError(\"Unexpected teacher parameter format:\", type(loaded), loaded.shape)\n",
    "\n",
    "print(\"Teacher param keys:\", list(params.keys()))  # should print ['normalizer', 'policy', 'value']\n",
    "\n",
    "# --- Network factory setup ---\n",
    "ppo_params = locomotion_params.brax_ppo_config('Go1JoystickRoughTerrain')\n",
    "ppo_params.network_factory['policy_obs_key'] = 'privileged_state'  \n",
    "network_factory = functools.partial(ppo_networks.make_ppo_networks, **ppo_params.network_factory)\n",
    "ppo_network = network_factory(\n",
    "    obs_shape=(96,),\n",
    "    action_size=env.action_size,\n",
    "    preprocess_observations_fn=running_statistics.normalize,\n",
    ")\n",
    "\n",
    "    \n",
    "# Create PPO network\n",
    "ppo_network = network_factory(\n",
    "    observation_size=(96,),\n",
    "    action_size=env.action_size,\n",
    "    preprocess_observations_fn=running_statistics.normalize,\n",
    ")\n",
    "\n",
    "# Create inference fn\n",
    "inference_params = (params['normalizer'], params['policy'])\n",
    "make_policy = ppo_networks.make_inference_fn(ppo_network)\n",
    "jit_inference_fn = jax.jit(make_policy(inference_params, deterministic=True))\n",
    "\n",
    "print(\"Teacher policy loaded and compiled successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "- Teacher is run for one episode at a time with privileged observations\n",
    "- Non-priveleged observations are saved as student input\n",
    "- action distribution (logits) are saved as student targets\n",
    "- RNN is trained with data from episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.custom_ppo_train import _maybe_wrap_env\n",
    "\n",
    "seed = 42\n",
    "\n",
    "action_size = 12\n",
    "hidden_size = (256, 128, 64)\n",
    "# hidden_size = 64\n",
    "\n",
    "episodes = 1000 # preivously 1000\n",
    "envs_per_episode = 1\n",
    "episode_length = 1024 # preivously 1024\n",
    "action_repeat = 1\n",
    "\n",
    "learning_rate = 1e-4 # preivously 1e-5\n",
    "\n",
    "teacher_visualisation = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Policy\n",
    "\n",
    "Experiment 1: training with a newly initialized LSTM for a Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student network initialized!\n",
      "Input shape: (128, 52)\n",
      "Expected output shape: (batch_size, 24) for logits\n",
      "Test output shape: (128, 24)\n"
     ]
    }
   ],
   "source": [
    "# Policy Definition\n",
    "from typing import Sequence\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "\n",
    "class StudentPolicy(nn.Module):\n",
    "    \"\"\"Feedforward student policy that mimics teacher's action distribution\"\"\"\n",
    "    action_size: int\n",
    "    hidden_size: Sequence[int]   # Number of units in the hidden layer\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # x shape: (batch_size, obs_dim)\n",
    "        \n",
    "        # # First dense layer with activation\n",
    "        # x = nn.Dense(self.hidden_size)(x)\n",
    "        # x = nn.relu(x)  # or use nn.tanh(x) if preferred\n",
    "        \n",
    "        for feat in self.hidden_size[:-1]:\n",
    "            x = nn.relu(nn.Dense(feat)(x))\n",
    "\n",
    "        # Second dense layer to output logits (actions + log_stds)\n",
    "        # logits = nn.Dense(features=2 * self.action_size)(x)\n",
    "        # # logits = nn.Dense(features=self.action_size)(x)\n",
    "        logits = nn.Dense(2 * self.action_size,\n",
    "                  kernel_init=nn.initializers.lecun_normal(),\n",
    "                  bias_init=nn.initializers.zeros)(x)\n",
    "        \n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize student network\n",
    "student_obs_dim = 52\n",
    "batch_size = 128\n",
    "episode_length = 1024\n",
    "batches = episode_length // batch_size\n",
    "\n",
    "# Create student network\n",
    "student_net = StudentPolicy(action_size = action_size, hidden_size = hidden_size)\n",
    "\n",
    "# Initialize with dummy data\n",
    "dummy_input = jnp.ones((batch_size, student_obs_dim))\n",
    "key_student = jax.random.PRNGKey(42)\n",
    "student_params = student_net.init(key_student, dummy_input)\n",
    "\n",
    "print(f\"Student network initialized!\")\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"Expected output shape: (batch_size, 24) for logits\")\n",
    "\n",
    "# Test student network\n",
    "test_output = student_net.apply(student_params, dummy_input)\n",
    "print(f\"Test output shape: {test_output.shape}\")\n",
    "# print(f\"Student test output means: {test_output[:, :action_size]}\")\n",
    "# print(f\"Student test output log stds: {test_output[:, action_size:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Teacher-Student Training\n",
    "Define the functions needed to extract logits from the teacher network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get logits from teacher network\n",
    "@jax.jit\n",
    "def get_teacher_logits(params, observations):\n",
    "    # Normalize obs using the saved normalizer\n",
    "    norm_obs = running_statistics.normalize(observations, params['normalizer'])\n",
    "    logits = ppo_network.policy_network.apply(params['policy'], norm_obs)\n",
    "    return logits\n",
    "\n",
    "\n",
    "# Training setup\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(student_params)\n",
    "\n",
    "# Training function\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, inputs, targets):\n",
    "    def loss_fn(params):\n",
    "        # predictions = student_net.apply(params, inputs)\n",
    "        # loss = jnp.mean((predictions - targets) ** 2)\n",
    "        # return loss\n",
    "\n",
    "        # Student prediction (logits)\n",
    "        student_logits = student_net.apply(params, inputs)\n",
    "\n",
    "        # Split logits into means and log_stds\n",
    "        student_means = student_logits[:, :action_size]\n",
    "        student_log_stds = student_logits[:, action_size:]\n",
    "\n",
    "        teacher_means = targets[:, :action_size]\n",
    "        teacher_log_stds = targets[:, action_size:]\n",
    "\n",
    "        # Clip log_stds for numerical stability\n",
    "        student_log_stds = jnp.clip(student_log_stds, -20, 2)\n",
    "        teacher_log_stds = jnp.clip(teacher_log_stds, -20, 2)\n",
    "        \n",
    "        # Convert log_stds to variances (more stable than stds)\n",
    "        student_var = jnp.exp(2 * student_log_stds)\n",
    "        teacher_var = jnp.exp(2 * teacher_log_stds)\n",
    "        \n",
    "        # KL(teacher || student)\n",
    "        kl_div = 0.5 * (\n",
    "            2 * (student_log_stds - teacher_log_stds) +\n",
    "            teacher_var / student_var +\n",
    "            jnp.square(teacher_means - student_means) / student_var - 1.0\n",
    "        )\n",
    "\n",
    "        # Sum over action dimensions and average over batch\n",
    "        loss = jnp.mean(jnp.sum(kl_div, axis=-1))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'DynamicJaxprTracer' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episode_length):\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Get teacher action and logits\u001b[39;00m\n\u001b[32m     55\u001b[39m     act_rng, rng = jax.random.split(rng)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     ctrl, _ = jit_inference_fn(state.obs[\u001b[33m'\u001b[39m\u001b[33mprivileged_state\u001b[39m\u001b[33m'\u001b[39m], act_rng)\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# print(f\"control after inference: {ctrl}\")\u001b[39;00m\n\u001b[32m     58\u001b[39m \n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# Get teacher logits (distribution parameters)\u001b[39;00m\n\u001b[32m     60\u001b[39m     logits = get_teacher_logits(params, state.obs[\u001b[33m'\u001b[39m\u001b[33mprivileged_state\u001b[39m\u001b[33m'\u001b[39m])\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/KTH/lib/python3.11/site-packages/brax/training/agents/ppo/networks.py:47\u001b[39m, in \u001b[36mmake_inference_fn.<locals>.make_policy.<locals>.policy\u001b[39m\u001b[34m(observations, key_sample)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpolicy\u001b[39m(\n\u001b[32m     44\u001b[39m     observations: types.Observation, key_sample: PRNGKey\n\u001b[32m     45\u001b[39m ) -> Tuple[types.Action, types.Extra]:\n\u001b[32m     46\u001b[39m   param_subset = (params[\u001b[32m0\u001b[39m], params[\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# normalizer and policy params\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m   logits = policy_network.apply(*param_subset, observations)\n\u001b[32m     48\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ppo_networks.parametric_action_distribution.mode(logits), {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/KTH/lib/python3.11/site-packages/brax/training/networks.py:356\u001b[39m, in \u001b[36mmake_policy_network.<locals>.apply\u001b[39m\u001b[34m(processor_params, policy_params, obs)\u001b[39m\n\u001b[32m    352\u001b[39m   obs = preprocess_observations_fn(\n\u001b[32m    353\u001b[39m       obs[obs_key], normalizer_select(processor_params, obs_key)\n\u001b[32m    354\u001b[39m   )\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m   obs = preprocess_observations_fn(obs, processor_params)\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m policy_module.apply(policy_params, obs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/KTH/lib/python3.11/site-packages/brax/training/acme/running_statistics.py:217\u001b[39m, in \u001b[36mnormalize\u001b[39m\u001b[34m(batch, mean_std, max_abs_value)\u001b[39m\n\u001b[32m    214\u001b[39m     data = jnp.clip(data, -max_abs_value, +max_abs_value)\n\u001b[32m    215\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m jax.tree_util.tree_map(normalize_leaf, batch, mean_std.mean, mean_std.std)\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/KTH/lib/python3.11/site-packages/brax/training/acme/running_statistics.py:211\u001b[39m, in \u001b[36mnormalize.<locals>.normalize_leaf\u001b[39m\u001b[34m(data, mean, std)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m jnp.issubdtype(data.dtype, jnp.inexact):\n\u001b[32m    210\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m data = (data - mean) / std\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_abs_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    213\u001b[39m   \u001b[38;5;66;03m# TODO(b/124318564): remove pylint directive\u001b[39;00m\n\u001b[32m    214\u001b[39m   data = jnp.clip(data, -max_abs_value, +max_abs_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/KTH/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:1141\u001b[39m, in \u001b[36m_forward_operator_to_aval.<locals>.op\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args):\n\u001b[32m-> \u001b[39m\u001b[32m1141\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.aval, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)(\u001b[38;5;28mself\u001b[39m, *args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/KTH/lib/python3.11/site-packages/jax/_src/numpy/array_methods.py:608\u001b[39m, in \u001b[36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n\u001b[32m--> \u001b[39m\u001b[32m608\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33munsupported operand type(s) for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mopchar\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[32m0\u001b[39m]).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(args[\u001b[32m1\u001b[39m]).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for -: 'DynamicJaxprTracer' and 'dict'"
     ]
    }
   ],
   "source": [
    "from mujoco_playground._src.gait import draw_joystick_command\n",
    "env_cfg = default_config()\n",
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [0.0, 0.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2*jp.pi]\n",
    "\n",
    "\n",
    "# Loop over episodes\n",
    "training_losses = []\n",
    "test_losses = []\n",
    "for episode in range(episodes):\n",
    "    # print(f\"\\n Episode {episode + 1}/{episodes} \")\n",
    "    \n",
    "    key = jax.random.PRNGKey(seed + episode)\n",
    "    key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "\n",
    "    wrapper_env = _maybe_wrap_env(\n",
    "        env,\n",
    "        wrap_env=True,\n",
    "        num_envs=envs_per_episode,\n",
    "        episode_length=episode_length,\n",
    "        action_repeat=action_repeat,\n",
    "        key_env=key_env,\n",
    "    )\n",
    "\n",
    "    # Reset environment \n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key_envs = jax.random.split(key_env, num_envs)\n",
    "    # env_state = reset_fn(key_envs)\n",
    "    env_state = reset_fn(key_env) # Single env\n",
    "\n",
    "    rng = jax.random.PRNGKey(episode)\n",
    "    # raw_command = jax.random.uniform(rng, shape=(3), minval=0.0, maxval=1.0)\n",
    "    raw_command = jp.array([0.5, 0.0, 0.0]) \n",
    "\n",
    "    command = jp.array([\n",
    "        raw_command[0] * env_cfg.command_config.a[0], \n",
    "        raw_command[1] * env_cfg.command_config.a[1],\n",
    "        raw_command[2] * env_cfg.command_config.a[2] \n",
    "    ])\n",
    "    state = jit_reset(rng)\n",
    "    state.info[\"command\"] = command\n",
    "\n",
    "    print(\"obs keys:\", list(state.obs.keys()))          # should include 'privileged_state' and 'state'\n",
    "    print(\"teacher param keys:\", list(params.keys()))    # should include 'normalizer','policy','value'\n",
    "\n",
    "    # Visualisation storing\n",
    "    rollout = []\n",
    "    modify_scene_fns = []\n",
    "\n",
    "    # Training data storing\n",
    "    student_inputs = []\n",
    "    student_targets = []\n",
    "    \n",
    "    for step in range(episode_length):\n",
    "        # Get teacher action and logits\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "        # print(f\"control after inference: {ctrl}\")\n",
    "\n",
    "        # Get teacher logits (distribution parameters)\n",
    "        logits = get_teacher_logits(params, state.obs)\n",
    "\n",
    "\n",
    "        # TO TEST IF CONVERSION WORKS\n",
    "        # Convert logits to action\n",
    "        # parametric_action_distribution = ppo_network.parametric_action_distribution\n",
    "        # raw_action = parametric_action_distribution.sample_no_postprocessing(logits, rng)\n",
    "        # ctrl = parametric_action_distribution.postprocess(raw_action)\n",
    "        # ctrl = ppo_network.parametric_action_distribution.mode(logits) # This seems to be the correct way to do it\n",
    "        # print(f\"control after conversion back and fourth: {ctrl}\")\n",
    "        \n",
    "        # Store data for student training\n",
    "        student_inputs.append(state.obs['state'])  # Non-privileged observations\n",
    "        student_targets.append(logits)  # Teacher's action distribution logits\n",
    "        # student_targets.append(ctrl)  # Teacher's action distribution ctrl\n",
    "        \n",
    "        # Step environment\n",
    "        state = jit_step(state, ctrl)\n",
    "        state.info[\"command\"] = command\n",
    "        \n",
    "        # Visualization data\n",
    "        rollout.append(state)\n",
    "        xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "        xyz += np.array([0, 0, 0.2])\n",
    "        x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "        yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "        modify_scene_fns.append(\n",
    "            functools.partial(\n",
    "                draw_joystick_command,\n",
    "                cmd=state.info[\"command\"],\n",
    "                xyz=xyz,\n",
    "                theta=yaw,\n",
    "                scl=abs(state.info[\"command\"][0]) / env_cfg.command_config.a[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Prepare training data\n",
    "    student_inputs_array = jnp.array(student_inputs)  # (1024, 52)\n",
    "    student_targets_array = jnp.array(student_targets)  # (1024, 24)\n",
    "        \n",
    "    # Train student\n",
    "    total_loss = 0\n",
    "    for batch_idx in range(batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        batch_inputs = student_inputs_array[start_idx:end_idx]\n",
    "        batch_targets = student_targets_array[start_idx:end_idx]\n",
    "        \n",
    "        student_params, opt_state, loss = train_step(\n",
    "            student_params, opt_state, batch_inputs, batch_targets\n",
    "        )\n",
    "        total_loss += loss\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode: {episode} | Loss: {total_loss/batches}\")\n",
    "    training_losses.append(total_loss / batches)\n",
    "\n",
    "    # Optional visualization\n",
    "    if teacher_visualisation: \n",
    "        render_every = 2\n",
    "        fps = 1.0 / env.dt / render_every\n",
    "        traj = rollout[::render_every]\n",
    "        mod_fns = modify_scene_fns[::render_every]\n",
    "\n",
    "        scene_option = mujoco.MjvOption()\n",
    "        scene_option.geomgroup[2] = True\n",
    "        scene_option.geomgroup[3] = False\n",
    "        scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "        scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "        scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "        frames = env.render(\n",
    "            traj,\n",
    "            camera=\"track\",\n",
    "            scene_option=scene_option,\n",
    "            width=640,\n",
    "            height=480,\n",
    "            modify_scene_fns=mod_fns,\n",
    "        )   \n",
    "        media.show_video(frames, fps=fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot training metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(range(1, len(training_losses) + 1), training_losses)\n",
    "plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"MSE Loss\")\n",
    "plt.ylabel(\"KL Divergence Loss\")\n",
    "plt.legend((\"Training Loss\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save student RNN parameters\n",
    "import pickle\n",
    "\n",
    "# Save\n",
    "with open('student_params_MLP_KL_1000episodes.pkl', 'wb') as f:\n",
    "    pickle.dump(student_params, f)\n",
    "print(\"Student saved to student_params_MLP_KL_1000episodes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (OPTIONAL): Load Trained RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load student RNN parameters\n",
    "with open('student_params_MLP_KL_1000episodes.pkl', 'rb') as f:\n",
    "    student_params = pickle.load(f)\n",
    "print(\"Student loaded from student_params_MLP_KL_1000episodes.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Student Polcicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Environment Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env config\n",
    "student_env_cfg = default_config()\n",
    "student_env_cfg.pert_config.enable = True\n",
    "student_env_cfg.pert_config.velocity_kick = [0.0, 0.0]\n",
    "student_env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "student_env_cfg.command_config.a = [1.5, 0.8, 2*jp.pi] # Max command values\n",
    "\n",
    "seed = 42\n",
    "\n",
    "num_episodes = 5\n",
    "episode_length = 500 # previously 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student eval\n",
    "from mujoco_playground._src.gait import draw_joystick_command\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    key = jax.random.PRNGKey(episode + seed)\n",
    "    key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "\n",
    "    wrapper_env = _maybe_wrap_env(\n",
    "    env,\n",
    "    wrap_env = True,\n",
    "    num_envs = 1,\n",
    "    episode_length = episode_length,\n",
    "    action_repeat = 1,\n",
    "    key_env = key_env,\n",
    "    )\n",
    "\n",
    "    # Reset environment \n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key_envs = jax.random.split(key_env, num_envs)\n",
    "    env_state = reset_fn(key_envs)\n",
    "\n",
    "    \n",
    "    # Set commands \n",
    "    rng = jax.random.PRNGKey(episode)\n",
    "    # raw_command = jax.random.uniform(rng, shape=(3), minval=0.0, maxval=1.0)\n",
    "    raw_command = jp.array([0.5, 0.0, 0.0])  # Hard coded for testing\n",
    "\n",
    "    command = jp.array([\n",
    "        raw_command[0] * student_env_cfg.command_config.a[0], \n",
    "        raw_command[1] * student_env_cfg.command_config.a[1],\n",
    "        raw_command[2] * student_env_cfg.command_config.a[2] \n",
    "    ])\n",
    "    state = jit_reset(rng)\n",
    "    state.info[\"command\"] = command\n",
    "\n",
    "    # Visualisation storing\n",
    "    rollout = []\n",
    "    modify_scene_fns = []\n",
    "\n",
    "    for step in range(episode_length):\n",
    "        # feed non-priveleged observations to network\n",
    "        student_obs = state.obs['state'] \n",
    "\n",
    "        # Get student logits and convert to action\n",
    "        student_obs_batch = student_obs[:student_obs_dim].reshape(1, -1)\n",
    "        logits = student_net.apply(student_params, student_obs_batch).squeeze(0)\n",
    "        ctrl = ppo_network.parametric_action_distribution.mode(logits)\n",
    "\n",
    "        # ctrl = student_net.apply(student_params, student_obs_batch).squeeze(0)\n",
    "\n",
    "        # Take step\n",
    "        state = jit_step(state, ctrl)\n",
    "        state.info[\"command\"] = command\n",
    "\n",
    "        # Visualization magic\n",
    "        rollout.append(state)\n",
    "        xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "        xyz += np.array([0, 0, 0.2])\n",
    "        x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "        yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "        modify_scene_fns.append(\n",
    "            functools.partial(\n",
    "                draw_joystick_command,\n",
    "                cmd=state.info[\"command\"],\n",
    "                xyz=xyz,\n",
    "                theta=yaw,\n",
    "                scl=abs(state.info[\"command\"][0]) / student_env_cfg.command_config.a[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Display visualisation magic\n",
    "    render_every = 2\n",
    "    fps = 1.0 / env.dt / render_every\n",
    "    traj = rollout[::render_every]\n",
    "    mod_fns = modify_scene_fns[::render_every]\n",
    "\n",
    "    scene_option = mujoco.MjvOption()\n",
    "    scene_option.geomgroup[2] = True\n",
    "    scene_option.geomgroup[3] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "    frames = env.render(\n",
    "        traj,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=640,\n",
    "        height=480,\n",
    "        modify_scene_fns=mod_fns,\n",
    "    )   \n",
    "    media.show_video(frames, fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
