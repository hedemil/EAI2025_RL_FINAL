{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 15:35:36.596825: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Visualization script for Go1 with height scanner.\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "# Tell XLA to use Triton GEMM\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "\n",
    "import mujoco\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from custom_env import Joystick, default_config\n",
    "\n",
    "from datetime import datetime\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import functools\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import Image as IPyimage, display, HTML, clear_output\n",
    "\n",
    "from utils import render_video_during_training, evaluate_policy\n",
    "\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True   # Show visual geoms\n",
    "scene_option.geomgroup[3] = False  # Hide collision geoms\n",
    "scene_option.geomgroup[5] = True   # Show sites (including height scanner visualization)\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Show contact points\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_RANGEFINDER] = True\n",
    "print(\"Creating Visualization...\")\n",
    "\n",
    "xml_path = 'custom_env.xml' # 'custom_env_debug_wall.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "\n",
    "# JIT compile the functions for speed\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_terrain_height = jax.jit(env._get_torso_terrain_height)\n",
    "\n",
    "seed = 1234\n",
    "num_envs = ()\n",
    "key = jax.random.PRNGKey(seed)\n",
    "key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "key_envs = jax.random.split(key_env, num_envs)\n",
    "env_state = jit_reset(key_envs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Policy\n",
    "\n",
    "- pretrained inside train.ipynb\n",
    "- we want to load the parameters\n",
    "\n",
    "- Inputs: privileged_state with heightmap\n",
    "- Output: action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_repeat': 1, 'batch_size': 256, 'discounting': 0.97, 'entropy_cost': 0.01, 'episode_length': 1000, 'learning_rate': 0.0003, 'max_grad_norm': 1.0, 'network_factory': policy_hidden_layer_sizes: &id001 !!python/tuple\n",
      "- 512\n",
      "- 256\n",
      "- 128\n",
      "policy_obs_key: state\n",
      "value_hidden_layer_sizes: *id001\n",
      "value_obs_key: privileged_state\n",
      ", 'normalize_observations': True, 'num_envs': 8192, 'num_evals': 10, 'num_minibatches': 32, 'num_resets_per_eval': 1, 'num_timesteps': 1, 'num_updates_per_batch': 4, 'reward_scaling': 1.0, 'unroll_length': 20}\n"
     ]
    }
   ],
   "source": [
    "import flax\n",
    "import optax\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.acme import specs\n",
    "from brax.training.acme import running_statistics\n",
    "from brax.training import types\n",
    "\n",
    "obs_shape = (52,)\n",
    "normalize = lambda x, y: x\n",
    "action_size = env.action_size\n",
    "\n",
    "ppo_params = locomotion_params.brax_ppo_config('Go1JoystickRoughTerrain')\n",
    "ppo_training_params = dict(ppo_params)\n",
    "\n",
    "ppo_training_params[\"num_timesteps\"] = 1\n",
    "print(ppo_training_params)\n",
    "\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "if \"network_factory\" in ppo_params:\n",
    "    del ppo_training_params[\"network_factory\"]\n",
    "    network_factory = functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    )\n",
    "\n",
    "ppo_network = network_factory(\n",
    "    obs_shape, action_size, preprocess_observations_fn=normalize\n",
    ")\n",
    "\n",
    "init_params = ppo_losses.PPONetworkParams(\n",
    "    policy=ppo_network.policy_network.init(key_policy),\n",
    "    value=ppo_network.value_network.init(key_value),\n",
    ")\n",
    "\n",
    "make_policy = ppo_networks.make_inference_fn(ppo_network)\n",
    "\n",
    "params = np.load(\"params.npy\", allow_pickle=True)\n",
    "\n",
    "jit_inference_fn = jax.jit(make_policy(params, deterministic=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Policy\n",
    "\n",
    "Experiment 1: training with a newly initialized LSTM for a Recurrent Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Definition\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "\n",
    "lstm = nn.RNN(nn.LSTMCell(features=64))\n",
    "x = jnp.ones((10, 50, 52)) # (batch, time, features)\n",
    "variables = lstm.init(jax.random.PRNGKey(0), x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_ppo_train import _maybe_wrap_env\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "episodes = 1\n",
    "envs_per_episode = 1\n",
    "episode_length = 10\n",
    "action_repeat = 1\n",
    "\n",
    "learning_rate = 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(episodes):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "\n",
    "    wrapper_env = _maybe_wrap_env(\n",
    "        env,\n",
    "        wrap_env=True,\n",
    "        num_envs=envs_per_episode,\n",
    "        episode_length=episode_length,\n",
    "        action_repeat=action_repeat,\n",
    "        key_env=key_env,\n",
    "    )\n",
    "\n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key_envs = jax.random.split(key_env, num_envs)\n",
    "    env_state = reset_fn(key_envs)\n",
    "    # print(f\"ENV STATE: {env_state.done}\")\n",
    "    obs_shape = jax.tree_util.tree_map(lambda x: x.shape[1:], env_state.obs)\n",
    "    # print(f\"OBS STATE: {env_state.obs}\")\n",
    "\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    command = jax.random.uniform(rng, shape=(3), minval=0.0, maxval=1.0)\n",
    "    state = jit_reset(rng)\n",
    "    state.info[\"command\"] = command\n",
    "\n",
    "    for _ in range(episode_length): # NOT DOING DONE: CHECK .done?\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "        state = jit_step(state, ctrl)\n",
    "        state.info[\"command\"] = command\n",
    "        \n",
    "    # if normalize_observations:\n",
    "    #     normalize = running_statistics.normalize\n",
    "\n",
    "    # optimizer = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "    # loss_fn = #\n",
    "\n",
    "    # gradient_update_fn = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PjitFunction of <function make_inference_fn.<locals>.make_policy.<locals>.policy at 0x7f0fc81071c0>>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jit_inference_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " done = False\n",
    "    \n",
    "while not done:\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "    done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
