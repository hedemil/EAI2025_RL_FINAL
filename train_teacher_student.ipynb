{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Visualization...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Visualization script for Go1 with height scanner.\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "# Tell XLA to use Triton GEMM\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "\n",
    "import mujoco\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from custom_env import Joystick, default_config\n",
    "\n",
    "from datetime import datetime\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import functools\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import Image as IPyimage, display, HTML, clear_output\n",
    "\n",
    "from utils import render_video_during_training, evaluate_policy\n",
    "\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True   # Show visual geoms\n",
    "scene_option.geomgroup[3] = False  # Hide collision geoms\n",
    "scene_option.geomgroup[5] = True   # Show sites (including height scanner visualization)\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Show contact points\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_RANGEFINDER] = True\n",
    "print(\"Creating Visualization...\")\n",
    "\n",
    "xml_path = 'custom_env.xml' # 'custom_env_debug_wall.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "\n",
    "# JIT compile the functions for speed\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_terrain_height = jax.jit(env._get_torso_terrain_height)\n",
    "\n",
    "seed = 1234\n",
    "num_envs = ()\n",
    "key = jax.random.PRNGKey(seed)\n",
    "key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "key_envs = jax.random.split(key_env, num_envs)\n",
    "env_state = jit_reset(key_envs)\n",
    "\n",
    "obs_size = jax.tree.map(lambda x: x.shape[1:], env_state.obs)\n",
    "action_size = env.action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Policy\n",
    "\n",
    "- pretrained inside train.ipynb\n",
    "- we want to load the parameters\n",
    "\n",
    "- Inputs: privileged_state with heightmap\n",
    "- Output: action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher obs_key: privileged_state hidden: (512, 256, 128)\n",
      "action shape: (12,) has raw_action: False\n"
     ]
    }
   ],
   "source": [
    "import functools, jax, jax.numpy as jnp, numpy as np\n",
    "from brax.training.acme import running_statistics\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from Teacher_Student.teacherOracle import TeacherOracle\n",
    "\n",
    "# 1) Load saved teacher params (normalizer, policy, value)\n",
    "teacher_normalizer, teacher_policy_params, teacher_value_params = np.load(\n",
    "    # Use teacher_params from train_teacher.ipynb since \n",
    "    # policy network is trained on privileged_state\n",
    "    'teacher_params.npy', allow_pickle=True\n",
    ").tolist()\n",
    "\n",
    "# 2) Infer the teacher’s hidden sizes from saved params\n",
    "pp = teacher_policy_params\n",
    "k0 = pp['params']['hidden_0']['kernel'].shape  # (in_dim, hidden0)\n",
    "policy_hidden = (\n",
    "    int(pp['params']['hidden_0']['kernel'].shape[1]),\n",
    "    int(pp['params']['hidden_1']['kernel'].shape[1]),\n",
    "    int(pp['params']['hidden_2']['kernel'].shape[1]),\n",
    ")\n",
    "in_dim = int(k0[0])\n",
    "\n",
    "# 3) Build observation_size mapping from the normalizer (robust even if obs_size is ()):\n",
    "#    This ensures networks are initialized with the exact trained feature dims.\n",
    "teacher_obs_sizes = {k: tuple(teacher_normalizer.mean[k].shape)\n",
    "                     for k in teacher_normalizer.mean}\n",
    "\n",
    "# Select which key the teacher used by matching input dim\n",
    "teacher_obs_key = None\n",
    "for k, shape in teacher_obs_sizes.items():\n",
    "    if len(shape) == 1 and int(shape[0]) == in_dim:\n",
    "        teacher_obs_key = k\n",
    "        break\n",
    "if teacher_obs_key is None:\n",
    "    # Fallback: prefer 'state' if present\n",
    "    teacher_obs_key = 'state' if 'state' in teacher_obs_sizes else next(iter(teacher_obs_sizes.keys()))\n",
    "\n",
    "print('Teacher obs_key:', teacher_obs_key, 'hidden:', policy_hidden)\n",
    "\n",
    "# 4) Rebuild teacher networks to match saved shapes\n",
    "teacher_nets = ppo_networks.make_ppo_networks(\n",
    "    observation_size=teacher_obs_sizes,          # use sizes from normalizer, not env obs\n",
    "    action_size=env.action_size,\n",
    "    preprocess_observations_fn=running_statistics.normalize,\n",
    "    policy_obs_key=teacher_obs_key,\n",
    "    value_obs_key=teacher_obs_key,\n",
    "    policy_hidden_layer_sizes=policy_hidden,\n",
    "    distribution_type='tanh_normal',\n",
    "    noise_std_type='log',\n",
    ")\n",
    "make_teacher_policy = ppo_networks.make_inference_fn(teacher_nets)\n",
    "\n",
    "# 5) Create the oracle (uses the teacher’s stochastic policy by default)\n",
    "teacher = TeacherOracle(\n",
    "    make_policy=make_teacher_policy,\n",
    "    normalizer_params=teacher_normalizer,\n",
    "    policy_params=teacher_policy_params,\n",
    "    value_params=teacher_value_params,\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "a, ex = teacher.act(env_state.obs, jax.random.PRNGKey(0))\n",
    "print('action shape:', a.shape, 'has raw_action:', 'raw_action' in ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Policy\n",
    "\n",
    "Experiment 1: training with a newly initialized LSTM for a Recurrent Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Definition\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "\n",
    "lstm = nn.RNN(nn.LSTMCell(features=64))\n",
    "x = jnp.ones((10, 50, 52)) # (batch, time, features)\n",
    "variables = lstm.init(jax.random.PRNGKey(0), x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_ppo_train import _maybe_wrap_env\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "episodes = 1\n",
    "envs_per_episode = 1\n",
    "episode_length = 10\n",
    "action_repeat = 1\n",
    "\n",
    "learning_rate = 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for episode in range(episodes):\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "\n",
    "    wrapper_env = _maybe_wrap_env(\n",
    "        env,\n",
    "        wrap_env=True,\n",
    "        num_envs=envs_per_episode,\n",
    "        episode_length=episode_length,\n",
    "        action_repeat=action_repeat,\n",
    "        key_env=key_env,\n",
    "    )\n",
    "\n",
    "    reset_fn = jax.jit(env.reset)\n",
    "    key_envs = jax.random.split(key_env, num_envs)\n",
    "    env_state = reset_fn(key_envs)\n",
    "    # print(f\"ENV STATE: {env_state.done}\")\n",
    "    obs_shape = jax.tree_util.tree_map(lambda x: x.shape[1:], env_state.obs)\n",
    "    # print(f\"OBS STATE: {env_state.obs}\")\n",
    "\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    command = jax.random.uniform(rng, shape=(3), minval=0.0, maxval=1.0)\n",
    "    state = jit_reset(rng)\n",
    "    state.info[\"command\"] = command\n",
    "\n",
    "    for _ in range(episode_length): # NOT DOING DONE: CHECK .done?\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        action, _ = teacher.act(state.obs, act_rng)\n",
    "        state = jit_step(state, action)\n",
    "        state.info[\"command\"] = command\n",
    "        \n",
    "    # if normalize_observations:\n",
    "    #     normalize = running_statistics.normalize\n",
    "\n",
    "    # optimizer = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "    # loss_fn = #\n",
    "\n",
    "    # gradient_update_fn = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_inference_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " done = False\n",
    "    \n",
    "while not done:\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "    done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
