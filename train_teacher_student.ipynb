{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Visualization script for Go1 with height scanner.\"\"\"\n",
    "import os\n",
    "import subprocess\n",
    "# Tell XLA to use Triton GEMM\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "\n",
    "import mujoco\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from custom_env import Joystick, default_config\n",
    "\n",
    "from datetime import datetime\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import functools\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import Image as IPyimage, display, HTML, clear_output\n",
    "\n",
    "from utils import render_video_during_training, evaluate_policy\n",
    "\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True   # Show visual geoms\n",
    "scene_option.geomgroup[3] = False  # Hide collision geoms\n",
    "scene_option.geomgroup[5] = True   # Show sites (including height scanner visualization)\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Show contact points\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_RANGEFINDER] = True\n",
    "print(\"Creating Visualization...\")\n",
    "\n",
    "xml_path = 'custom_env.xml' # 'custom_env_debug_wall.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "\n",
    "# JIT compile the functions for speed\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_terrain_height = jax.jit(env._get_torso_terrain_height)\n",
    "\n",
    "seed = 1234\n",
    "num_envs = ()\n",
    "key = jax.random.PRNGKey(seed)\n",
    "key, key_env, eval_key, key_policy, key_value = jax.random.split(key, 5)\n",
    "key_envs = jax.random.split(key_env, num_envs)\n",
    "env_state = jit_reset(key_envs)\n",
    "\n",
    "obs_size = jax.tree.map(lambda x: x.shape[1:], env_state.obs)\n",
    "action_size = env.action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Policy\n",
    "\n",
    "- pretrained inside train.ipynb\n",
    "- we want to load the parameters\n",
    "\n",
    "- Inputs: privileged_state with heightmap\n",
    "- Output: action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools, jax, jax.numpy as jnp, numpy as np\n",
    "from brax.training.acme import running_statistics\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from Teacher_Student.teacherOracle import TeacherOracle\n",
    "\n",
    "# 1) Load saved teacher params (normalizer, policy, value)\n",
    "teacher_normalizer, teacher_policy_params, teacher_value_params = np.load(\n",
    "    # Use teacher_params from train_teacher.ipynb since \n",
    "    # policy network is trained on privileged_state\n",
    "    'teacher_params.npy', allow_pickle=True\n",
    ").tolist()\n",
    "\n",
    "# 2) Infer the teacher’s hidden sizes from saved params\n",
    "pp = teacher_policy_params\n",
    "k0 = pp['params']['hidden_0']['kernel'].shape  # (in_dim, hidden0)\n",
    "policy_hidden = (\n",
    "    int(pp['params']['hidden_0']['kernel'].shape[1]),\n",
    "    int(pp['params']['hidden_1']['kernel'].shape[1]),\n",
    "    int(pp['params']['hidden_2']['kernel'].shape[1]),\n",
    ")\n",
    "in_dim = int(k0[0])\n",
    "\n",
    "# 3) Build observation_size mapping from the normalizer (robust even if obs_size is ()):\n",
    "#    This ensures networks are initialized with the exact trained feature dims.\n",
    "teacher_obs_sizes = {k: tuple(teacher_normalizer.mean[k].shape)\n",
    "                     for k in teacher_normalizer.mean}\n",
    "\n",
    "# Select which key the teacher used by matching input dim\n",
    "teacher_obs_key = None\n",
    "for k, shape in teacher_obs_sizes.items():\n",
    "    if len(shape) == 1 and int(shape[0]) == in_dim:\n",
    "        teacher_obs_key = k\n",
    "        break\n",
    "if teacher_obs_key is None:\n",
    "    # Fallback: prefer 'state' if present\n",
    "    teacher_obs_key = 'state' if 'state' in teacher_obs_sizes else next(iter(teacher_obs_sizes.keys()))\n",
    "\n",
    "print('Teacher obs_key:', teacher_obs_key, 'hidden:', policy_hidden)\n",
    "\n",
    "# 4) Rebuild teacher networks to match saved shapes\n",
    "teacher_nets = ppo_networks.make_ppo_networks(\n",
    "    observation_size=teacher_obs_sizes,          # use sizes from normalizer, not env obs\n",
    "    action_size=env.action_size,\n",
    "    preprocess_observations_fn=running_statistics.normalize,\n",
    "    policy_obs_key=teacher_obs_key,\n",
    "    value_obs_key=teacher_obs_key,\n",
    "    policy_hidden_layer_sizes=policy_hidden,\n",
    "    distribution_type='tanh_normal',\n",
    "    noise_std_type='log',\n",
    ")\n",
    "make_teacher_policy = ppo_networks.make_inference_fn(teacher_nets)\n",
    "\n",
    "# 5) Create the oracle (uses the teacher’s stochastic policy by default)\n",
    "teacher = TeacherOracle(\n",
    "    make_policy=make_teacher_policy,\n",
    "    normalizer_params=teacher_normalizer,\n",
    "    policy_params=teacher_policy_params,\n",
    "    value_params=teacher_value_params,\n",
    ")\n",
    "\n",
    "# Quick check\n",
    "a, ex = teacher.act(env_state.obs, jax.random.PRNGKey(0))\n",
    "print('action shape:', a.shape, 'has raw_action:', 'raw_action' in ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Policy\n",
    "\n",
    "Student policy using Teacher_Student/studentPolicy.py with tanh-Normal head.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student policy built via Teacher_Student.studentPolicy\n",
    "import jax, jax.numpy as jnp\n",
    "from flax import linen\n",
    "from brax.training.acme import running_statistics\n",
    "from Teacher_Student.studentPolicy import make_student_policy, init_student_normalizer\n",
    "\n",
    "# Derive student obs size robustly even if no batch axis is present\n",
    "student_obs_dim = int(env_state.obs['state'].shape[-1])\n",
    "student_obs_sizes = {'state': (student_obs_dim,)}\n",
    "\n",
    "student = make_student_policy(\n",
    "    obs_size=student_obs_sizes,\n",
    "    action_size=env.action_size,\n",
    "    preprocess_fn=running_statistics.normalize,\n",
    "    hidden=(128, 128),\n",
    "    activation=linen.swish,\n",
    "    obs_key='state',\n",
    ")\n",
    "\n",
    "key, key_student = jax.random.split(key)\n",
    "student_policy_params = student.policy.init(key_student)\n",
    "student_normalizer_params = init_student_normalizer(env_state.obs['state'].shape[-1])\n",
    "\n",
    "def make_student_inference_fn(student):\n",
    "    def make_policy(params, deterministic=False):\n",
    "        def policy(observations, key_sample):\n",
    "            logits = student.policy.apply(params[0], params[1], observations)\n",
    "            if deterministic:\n",
    "                return student.parametric.mode(logits), {}\n",
    "            raw = student.parametric.sample_no_postprocessing(logits, key_sample)\n",
    "            logp = student.parametric.log_prob(logits, raw)\n",
    "            act = student.parametric.postprocess(raw)\n",
    "            return act, {'log_prob': logp, 'raw_action': raw}\n",
    "        return policy\n",
    "    return make_policy\n",
    "\n",
    "make_student_policy_inference = make_student_inference_fn(student)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from brax.training.acme import running_statistics\n",
    "\n",
    "optimizer = optax.adam(learning_rate=1e-4)\n",
    "opt_state = optimizer.init(student_policy_params)\n",
    "entropy_coef = 1e-3\n",
    "\n",
    "teacher_dist = teacher_nets.parametric_action_distribution\n",
    "\n",
    "def distill_step_loss(student_params, student_norm, obs, rng):\n",
    "    t_logits = teacher_nets.policy_network.apply(teacher_normalizer, teacher_policy_params, obs)\n",
    "    s_logits = student.policy.apply(student_norm, student_params, obs)\n",
    "    rng, rng_sample, rng_ent = jax.random.split(rng, 3)\n",
    "    raw_a = teacher_dist.sample_no_postprocessing(t_logits, rng_sample)\n",
    "    logp_t = teacher_dist.log_prob(t_logits, raw_a)\n",
    "    logp_s = student.parametric.log_prob(s_logits, raw_a)\n",
    "    kl = jnp.mean(logp_t - logp_s)\n",
    "    ent_s = jnp.mean(student.parametric.entropy(s_logits, rng_ent))\n",
    "    loss = kl - entropy_coef * ent_s\n",
    "    return loss, {'kl': kl, 'entropy': ent_s}\n",
    "\n",
    "@jax.jit\n",
    "def distill_step(student_params, student_norm, obs, rng, opt_state):\n",
    "    # Use value_and_grad to get both loss and gradients in one pass\n",
    "    (loss, metrics), grads = jax.value_and_grad(distill_step_loss, has_aux=True)(\n",
    "        student_params, student_norm, obs, rng\n",
    "    )\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, student_params)\n",
    "    new_params = optax.apply_updates(student_params, updates)\n",
    "    return new_params, opt_state, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_ppo_train import _maybe_wrap_env\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "episodes = 1\n",
    "envs_per_episode = 1\n",
    "episode_length = 10\n",
    "action_repeat = 1\n",
    "\n",
    "learning_rate = 1e-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "episodes = 25\n",
    "steps_per_episode = 100000000\n",
    "\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "reset_fn = jax.jit(env.reset)\n",
    "step_fn = jax.jit(env.step)\n",
    "\n",
    "episode_mean_kls = []\n",
    "episode_mean_entropy = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    rng, key_env = jax.random.split(rng)\n",
    "    state = reset_fn(key_env)\n",
    "    rng, key_cmd = jax.random.split(rng)\n",
    "    command = jax.random.uniform(key_cmd, shape=(3,), minval=0.0, maxval=1.0)\n",
    "    state.info['command'] = command\n",
    "    ep_kls = []\n",
    "    ep_ents = []\n",
    "    for t in range(steps_per_episode):\n",
    "        rng, key_act, key_distill = jax.random.split(rng, 3)\n",
    "        action, _ = teacher.act(state.obs, key_act)\n",
    "        next_state = step_fn(state, action)\n",
    "        next_state.info['command'] = command\n",
    "        student_normalizer_params = running_statistics.update(\n",
    "            student_normalizer_params, {'state': state.obs['state']}\n",
    "        )\n",
    "        student_policy_params, opt_state, m = distill_step(\n",
    "            student_policy_params, student_normalizer_params, state.obs, key_distill, opt_state\n",
    "        )\n",
    "        ep_kls.append(m['kl'])\n",
    "        ep_ents.append(m['entropy'])\n",
    "        state = next_state\n",
    "    mean_kl = float(jnp.mean(jnp.array(ep_kls)))\n",
    "    mean_ent = float(jnp.mean(jnp.array(ep_ents)))\n",
    "    episode_mean_kls.append(mean_kl)\n",
    "    episode_mean_entropy.append(mean_ent)\n",
    "    if ep % 1000 == 0:\n",
    "        print(f'Episode {ep}: mean KL={mean_kl:.4f} | mean entropy={mean_ent:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean KL and entropy per episode\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(episode_mean_kls, label='Mean KL per episode')\n",
    "plt.plot(episode_mean_entropy, label='Mean entropy per episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Distillation Progress')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, evaluate the comparison\n",
    "from utils import evaluate_policy_comparison\n",
    "\n",
    "# Create inference functions\n",
    "teacher_inference_fn = jax.jit(teacher._make_policy((teacher_normalizer, teacher_policy_params), deterministic=True))\n",
    "student_inference_fn = jax.jit(make_student_policy_inference((student_normalizer_params, student_policy_params), deterministic=True))\n",
    "\n",
    "# Evaluate side by side\n",
    "evaluate_policy_comparison(\n",
    "    env=env,\n",
    "    teacher_inference_fn=teacher_inference_fn,\n",
    "    student_inference_fn=student_inference_fn,\n",
    "    jit_step=jit_step,\n",
    "    jit_reset=jit_reset,\n",
    "    env_cfg=default_config(),\n",
    "    eval_env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
