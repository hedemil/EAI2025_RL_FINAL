{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3457292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Visualization script for Go1 with height scanner.\"\"\"\n",
    "import os\n",
    "# Tell XLA to use Triton GEMM\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "import numpy as np\n",
    "import mujoco\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import Image as IPyimage, display\n",
    "from custom_env import Joystick, default_config\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2838a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up visualization options\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True   # Show visual geoms\n",
    "scene_option.geomgroup[3] = False  # Hide collision geoms\n",
    "scene_option.geomgroup[5] = True   # Show sites (including height scanner visualization)\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Show contact points\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_RANGEFINDER] = True\n",
    "print(\"Creating Visualization...\")\n",
    "\n",
    "# We will no longer use the registry, but directly load our custom XML and model.\n",
    "# We will load the \"debug\" version here, which adds an additional wall *under* the robot\n",
    "# We add this to understand that changing walls correctly affects collision + raycasting.\n",
    "xml_path = 'custom_env.xml' # 'custom_env_debug_wall.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "\n",
    "# NOTE: For this test, we manually set init_q z position high to avoid collisions with walls \n",
    "# env._init_q = env._init_q.at[2].set(1.0)\n",
    "\n",
    "# JIT compile the functions for speed\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_terrain_height = jax.jit(env._get_torso_terrain_height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Any, Tuple\n",
    "from brax.training import types\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.types import Params\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from brax.training.agents.ppo.losses import compute_gae\n",
    "\n",
    "# Helper Struct for policy and value params\n",
    "@flax.struct.dataclass\n",
    "class PPONetworkParams:\n",
    "  \"\"\"Contains training state for the learner.\"\"\"\n",
    "\n",
    "  policy: Params\n",
    "  value: Params\n",
    "\n",
    "# PPO loss function to be implemented\n",
    "def compute_custom_ppo_loss(\n",
    "    params: PPONetworkParams,\n",
    "    normalizer_params: Any,\n",
    "    data: types.Transition,\n",
    "    rng: jnp.ndarray,\n",
    "    ppo_network: ppo_networks.PPONetworks,\n",
    "    entropy_cost: float = 1e-4,\n",
    "    discounting: float = 0.9,\n",
    "    reward_scaling: float = 1.0,\n",
    "    gae_lambda: float = 0.95,\n",
    "    clipping_epsilon: float = 0.3,\n",
    "    normalize_advantage: bool = True,\n",
    ") -> Tuple[jnp.ndarray, types.Metrics]:\n",
    "  \"\"\"Computes PPO loss.\n",
    "\n",
    "  Args:\n",
    "    params: Network parameters,\n",
    "    normalizer_params: Parameters of the normalizer.\n",
    "    data: Transition that with leading dimension [B, T]. extra fields required\n",
    "      are ['state_extras']['truncation'] ['policy_extras']['raw_action']\n",
    "      ['policy_extras']['log_prob']\n",
    "    rng: Random key\n",
    "    ppo_network: PPO networks.\n",
    "    entropy_cost: entropy cost.\n",
    "    discounting: discounting,\n",
    "    reward_scaling: reward multiplier.\n",
    "    gae_lambda: General advantage estimation lambda.\n",
    "    clipping_epsilon: Policy loss clipping epsilon\n",
    "    normalize_advantage: whether to normalize advantage estimate\n",
    "\n",
    "  Returns:\n",
    "    A tuple (loss, metrics)\n",
    "  \"\"\"\n",
    "  parametric_action_distribution = ppo_network.parametric_action_distribution\n",
    "  policy_apply = ppo_network.policy_network.apply\n",
    "  value_apply = ppo_network.value_network.apply\n",
    "\n",
    "  # Put the time dimension first.\n",
    "  data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)\n",
    "  policy_logits = policy_apply(\n",
    "      normalizer_params, params.policy, data.observation\n",
    "  )\n",
    "\n",
    "  baseline = value_apply(normalizer_params, params.value, data.observation)\n",
    "  terminal_obs = jax.tree_util.tree_map(lambda x: x[-1], data.next_observation)\n",
    "  bootstrap_value = value_apply(normalizer_params, params.value, terminal_obs)\n",
    "\n",
    "  rewards = data.reward * reward_scaling\n",
    "  truncation = data.extras['state_extras']['truncation']\n",
    "  termination = (1 - data.discount) * (1 - truncation)\n",
    "\n",
    "  target_action_log_probs = parametric_action_distribution.log_prob(\n",
    "      policy_logits, data.extras['policy_extras']['raw_action']\n",
    "  )\n",
    "  behaviour_action_log_probs = data.extras['policy_extras']['log_prob']\n",
    "\n",
    "  vs, advantages = compute_gae(\n",
    "      truncation=truncation,\n",
    "      termination=termination,\n",
    "      rewards=rewards,\n",
    "      values=baseline,\n",
    "      bootstrap_value=bootstrap_value,\n",
    "      lambda_=gae_lambda,\n",
    "      discount=discounting,\n",
    "  )\n",
    "  if normalize_advantage:\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "  \n",
    "  # TODO: Compute probability ratio between new and old policy\n",
    "  # Hint: Use jnp.exp() and compute the difference between log probabilities\n",
    "  rho_s = jnp.exp(target_action_log_probs - behaviour_action_log_probs)\n",
    "  \n",
    "  # TODO: Implement the PPO clipped surrogate objective\n",
    "  # Hint: Compare unclipped (rho_s * advantages) vs clipped version\n",
    "  # Use jnp.clip() with clipping_epsilon parameter\n",
    "  surrogate_loss1 = rho_s * advantages\n",
    "  surrogate_loss2 = jnp.clip(rho_s, 1 - clipping_epsilon, 1 + clipping_epsilon) * advantages\n",
    "\n",
    "  # TODO: PPO policy loss is the negative mean of the minimum of both surrogates\n",
    "  policy_loss = -jnp.mean(jnp.minimum(surrogate_loss1, surrogate_loss2))\n",
    "\n",
    "  # TODO: Implement value function loss\n",
    "  # Hint: Mean squared error between vs (targets) and baseline (predictions)\n",
    "  # Scale by 0.5 * 0.5 as in the original implementation\n",
    "  v_error =(vs - baseline)**2\n",
    "  v_loss = 0.25*jnp.mean(v_error) # 0.5 seems to be faster\n",
    "\n",
    "  # TODO: Implement entropy loss for exploration\n",
    "  # Hint: Use parametric_action_distribution.entropy() and multiply by entropy_cost\n",
    "  # Make it negative since we want to maximize entropy (minimize negative entropy)\n",
    "  entropy = parametric_action_distribution.entropy(policy_logits, rng)\n",
    "  entropy_loss = -entropy_cost*jnp.mean(entropy)\n",
    "\n",
    "  # TODO: Combine all loss components\n",
    "  total_loss = policy_loss + v_loss + entropy_loss\n",
    "  \n",
    "  return total_loss, {\n",
    "      'total_loss': total_loss,\n",
    "      'policy_loss': policy_loss,\n",
    "      'v_loss': v_loss,\n",
    "      'entropy_loss': entropy_loss,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef895140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import functools\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from IPython.display import HTML, clear_output\n",
    "import mujoco\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "import cv2\n",
    "import custom_ppo_train\n",
    "from utils import render_video_during_training, evaluate_policy\n",
    "import mediapy as media\n",
    "\n",
    "\n",
    "\n",
    "env_cfg = default_config()\n",
    "\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "# Store the current policy for video rendering\n",
    "current_policy = None\n",
    "\n",
    "num_timesteps = 10\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    times.append(datetime.now())\n",
    "    x_data.append(num_steps)\n",
    "    y_data.append(metrics[\"eval/episode_reward\"])\n",
    "    y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
    "\n",
    "    plt.xlim([0, num_timesteps * 1.25])\n",
    "    plt.xlabel(\"# environment steps\")\n",
    "    plt.ylabel(\"reward per episode\")\n",
    "    plt.title(f\"y={y_data[-1]:.3f}\")\n",
    "    plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n",
    "    \n",
    "    # Save the plot to a file instead of displaying it\n",
    "    display(plt.gcf())\n",
    "\n",
    "    # Render video if we have a current policy\n",
    "    if current_policy is not None:\n",
    "        render_video_during_training(current_policy, num_steps, jit_step, jit_reset, env_cfg, env)\n",
    "\n",
    "ppo_params = locomotion_params.brax_ppo_config('Go1JoystickRoughTerrain')\n",
    "ppo_training_params = dict(ppo_params)\n",
    "\n",
    "ppo_training_params[\"num_evals\"] = 0\n",
    "ppo_training_params[\"num_timesteps\"] = 10\n",
    "ppo_training_params[\"episode_length\"] = 10\n",
    "\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "\n",
    "if \"network_factory\" in ppo_params:\n",
    "    del ppo_training_params[\"network_factory\"]\n",
    "    ppo_params.network_factory['policy_obs_key'] = 'privileged_state'\n",
    "    network_factory = functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    )\n",
    "print(ppo_training_params)\n",
    "\n",
    "\n",
    "# Create a policy parameters callback to capture the current policy\n",
    "def policy_params_callback(_, make_policy_fn, params):\n",
    "    # Update the current policy for video rendering\n",
    "    global current_policy\n",
    "    current_policy = make_policy_fn(params, deterministic=True)\n",
    "    \n",
    "train_fn = functools.partial(\n",
    "        custom_ppo_train.train,\n",
    "        **ppo_training_params,\n",
    "        network_factory=network_factory,\n",
    "        # randomization_fn=randomizer,\n",
    "        progress_fn=progress,\n",
    "        policy_params_fn=policy_params_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d261bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "make_policy, params, _ = train_fn(environment=env,\n",
    "                                  eval_env=env,\n",
    "                                  wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "                                  compute_custom_ppo_loss_fn=compute_custom_ppo_loss\n",
    "                                 )\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [3.0, 6.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2*jp.pi]\n",
    "eval_env = env\n",
    "velocity_kick_range = [0.0, 0.0]  # Disable velocity kick.\n",
    "kick_duration_range = [0.05, 0.2]\n",
    "\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "jit_inference_fn = jax.jit(make_policy(params, deterministic=True))\n",
    "\n",
    "evaluate_policy(\n",
    "    eval_env,\n",
    "    jit_inference_fn,\n",
    "    jit_step,\n",
    "    jit_reset,\n",
    "    env_cfg,\n",
    "    eval_env,\n",
    "    velocity_kick_range,\n",
    "    kick_duration_range,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- SAVE ---\n",
    "def save_state_npy(params, path=\"teacher_params.npy\"):\n",
    "    np.save(path, params, allow_pickle=True)\n",
    "\n",
    "save_state_npy(params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
