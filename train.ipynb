{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3457292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Visualization script for Go1 with height scanner.\"\"\"\n",
    "import os\n",
    "# Tell XLA to use Triton GEMM\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "import numpy as np\n",
    "import mujoco\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import Image as IPyimage, display\n",
    "from custom_env import Joystick, default_config\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2838a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up visualization options\n",
    "scene_option = mujoco.MjvOption()\n",
    "scene_option.geomgroup[2] = True   # Show visual geoms\n",
    "scene_option.geomgroup[3] = False  # Hide collision geoms\n",
    "scene_option.geomgroup[5] = True   # Show sites (including height scanner visualization)\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True  # Show contact points\n",
    "scene_option.flags[mujoco.mjtVisFlag.mjVIS_RANGEFINDER] = True\n",
    "print(\"Creating Visualization...\")\n",
    "\n",
    "# We will no longer use the registry, but directly load our custom XML and model.\n",
    "# We will load the \"debug\" version here, which adds an additional wall *under* the robot\n",
    "# We add this to understand that changing walls correctly affects collision + raycasting.\n",
    "xml_path = 'custom_env.xml' # 'custom_env_debug_wall.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "\n",
    "# NOTE: For this test, we manually set init_q z position high to avoid collisions with walls \n",
    "# env._init_q = env._init_q.at[2].set(1.0)\n",
    "\n",
    "# JIT compile the functions for speed\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "jit_terrain_height = jax.jit(env._get_torso_terrain_height)\n",
    "\n",
    "# Initialize\n",
    "key = jax.random.PRNGKey(15)\n",
    "\n",
    "print(\"Running simulation...\")\n",
    "\n",
    "# Perform multiple random resets to show different wall configurations\n",
    "num_resets = 1\n",
    "steps_per_reset = 50  # More steps to show movement\n",
    "\n",
    "for reset_idx in range(num_resets):\n",
    "    print(f\"Creating GIF {reset_idx+1}/{num_resets}\")\n",
    "\n",
    "    # Randomize wall heights for this reset\n",
    "    # set_wall_heights(env, range_min=0.05, range_max=0.4)\n",
    "\n",
    "    # Generate new random key for this reset\n",
    "    key, reset_key = jax.random.split(key)\n",
    "\n",
    "    # Reset to new random position\n",
    "    state = jit_reset(reset_key)\n",
    "\n",
    "    # Collect frames and terrain heights for this reset\n",
    "    rollout = []\n",
    "    terrain_heights = []\n",
    "\n",
    "    for step in range(steps_per_reset):\n",
    "        rollout.append(state)\n",
    "\n",
    "        # Calculate terrain height for this state\n",
    "        terrain_height = jit_terrain_height(state.data)\n",
    "        terrain_heights.append(float(terrain_height))\n",
    "\n",
    "        # Check for NaN/inf in reward\n",
    "        if not jp.isfinite(state.reward):\n",
    "            print(f\"    WARNING: Non-finite reward detected!\")\n",
    "            print(f\"    Reward value: {state.reward}\")\n",
    "            raise ValueError(\"Non-finite reward encountered during simulation\")\n",
    "\n",
    "        # Use small random actions to show some movement\n",
    "        action_key, key = jax.random.split(key)\n",
    "        action = jax.random.normal(action_key, (env.action_size,)) * 0.1\n",
    "        state = jit_step(state, action)\n",
    "\n",
    "    # Debug prints: check wall positions from simulation data\n",
    "    current_data = rollout[-1].data  # Get the latest state\n",
    "    print(f\"\\nReset {reset_idx+1} - Wall positions:\")\n",
    "    for i, wall_geom_id in enumerate(env._wall_geom_ids):\n",
    "        wall_pos = current_data.geom_xpos[wall_geom_id]\n",
    "        print(f\"  Wall {i}: geom_xpos[z]={wall_pos[2]:.3f}\")\n",
    "    print()\n",
    "\n",
    "    # Render frames for this reset\n",
    "    print(f\"    Rendering {len(rollout)} frames...\")\n",
    "\n",
    "    frames = env.render(\n",
    "        rollout,\n",
    "        camera=\"track\",  # Use tracking camera\n",
    "        scene_option=scene_option,\n",
    "        width=480,\n",
    "        height=360,\n",
    "    )\n",
    "\n",
    "    # # Create GIF for this reset\n",
    "    # gif_filename = f'wall_randomization_{reset_idx+1:02d}.gif'\n",
    "    # print(f\"    Saving GIF to {gif_filename}...\")\n",
    "\n",
    "    # # Convert frames to PIL Images and add terrain height overlay\n",
    "    # pil_images = []\n",
    "    # for i, frame in enumerate(frames):\n",
    "    #     pil_image = Image.fromarray(frame)\n",
    "\n",
    "    #     # Add terrain height overlay\n",
    "    #     draw = ImageDraw.Draw(pil_image)\n",
    "    #     terrain_height_text = f\"Terrain Height: {terrain_heights[i]:.3f}m\"\n",
    "\n",
    "    #     # Try to use a default font, fallback to default if not available\n",
    "    #     try:\n",
    "    #         font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 20)\n",
    "    #     except:\n",
    "    #         font = ImageFont.load_default()\n",
    "\n",
    "    #     # Draw background rectangle for better text visibility\n",
    "    #     text_bbox = draw.textbbox((0, 0), terrain_height_text, font=font)\n",
    "    #     text_width = text_bbox[2] - text_bbox[0]\n",
    "    #     text_height = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "    #     # Position text in top-left corner with padding\n",
    "    #     x, y = 10, 10\n",
    "    #     draw.rectangle([x-5, y-5, x+text_width+5, y+text_height+5], fill=(0, 0, 0, 128))\n",
    "    #     draw.text((x, y), terrain_height_text, fill=(255, 255, 255), font=font)\n",
    "\n",
    "    #     pil_images.append(pil_image)\n",
    "    pil_images = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        pil_image = Image.fromarray(frame)\n",
    "\n",
    "        # Add terrain height overlay\n",
    "        draw = ImageDraw.Draw(pil_image)\n",
    "        terrain_height_text = f\"Terrain Height: {terrain_heights[i]:.3f}m\"\n",
    "\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 20)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        text_bbox = draw.textbbox((0, 0), terrain_height_text, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "        x, y = 10, 10\n",
    "        draw.rectangle([x-5, y-5, x+text_width+5, y+text_height+5], fill=(0, 0, 0, 128))\n",
    "        draw.text((x, y), terrain_height_text, fill=(255, 255, 255), font=font)\n",
    "\n",
    "        pil_images.append(pil_image)\n",
    "\n",
    "    # Display GIF inline without saving\n",
    "    buf = BytesIO()\n",
    "    pil_images[0].save(\n",
    "        buf,\n",
    "        format=\"GIF\",\n",
    "        save_all=True,\n",
    "        append_images=pil_images[1:],\n",
    "        duration=80,   # ms per frame\n",
    "        loop=0\n",
    "    )\n",
    "    buf.seek(0)\n",
    "    display(IPyimage(data=buf.getvalue()))\n",
    "    \n",
    "\n",
    "print(f\"âœ“ All {num_resets} GIFs created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46ca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Any, Tuple\n",
    "from brax.training import types\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.types import Params\n",
    "import flax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from brax.training.agents.ppo.losses import compute_gae\n",
    "\n",
    "# Helper Struct for policy and value params\n",
    "@flax.struct.dataclass\n",
    "class PPONetworkParams:\n",
    "  \"\"\"Contains training state for the learner.\"\"\"\n",
    "\n",
    "  policy: Params\n",
    "  value: Params\n",
    "\n",
    "# PPO loss function to be implemented\n",
    "def compute_custom_ppo_loss(\n",
    "    params: PPONetworkParams,\n",
    "    normalizer_params: Any,\n",
    "    data: types.Transition,\n",
    "    rng: jnp.ndarray,\n",
    "    ppo_network: ppo_networks.PPONetworks,\n",
    "    entropy_cost: float = 1e-4,\n",
    "    discounting: float = 0.9,\n",
    "    reward_scaling: float = 1.0,\n",
    "    gae_lambda: float = 0.95,\n",
    "    clipping_epsilon: float = 0.3,\n",
    "    normalize_advantage: bool = True,\n",
    ") -> Tuple[jnp.ndarray, types.Metrics]:\n",
    "  \"\"\"Computes PPO loss.\n",
    "\n",
    "  Args:\n",
    "    params: Network parameters,\n",
    "    normalizer_params: Parameters of the normalizer.\n",
    "    data: Transition that with leading dimension [B, T]. extra fields required\n",
    "      are ['state_extras']['truncation'] ['policy_extras']['raw_action']\n",
    "      ['policy_extras']['log_prob']\n",
    "    rng: Random key\n",
    "    ppo_network: PPO networks.\n",
    "    entropy_cost: entropy cost.\n",
    "    discounting: discounting,\n",
    "    reward_scaling: reward multiplier.\n",
    "    gae_lambda: General advantage estimation lambda.\n",
    "    clipping_epsilon: Policy loss clipping epsilon\n",
    "    normalize_advantage: whether to normalize advantage estimate\n",
    "\n",
    "  Returns:\n",
    "    A tuple (loss, metrics)\n",
    "  \"\"\"\n",
    "  parametric_action_distribution = ppo_network.parametric_action_distribution\n",
    "  policy_apply = ppo_network.policy_network.apply\n",
    "  value_apply = ppo_network.value_network.apply\n",
    "\n",
    "  # Put the time dimension first.\n",
    "  data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 0, 1), data)\n",
    "  policy_logits = policy_apply(\n",
    "      normalizer_params, params.policy, data.observation\n",
    "  )\n",
    "\n",
    "  baseline = value_apply(normalizer_params, params.value, data.observation)\n",
    "  terminal_obs = jax.tree_util.tree_map(lambda x: x[-1], data.next_observation)\n",
    "  bootstrap_value = value_apply(normalizer_params, params.value, terminal_obs)\n",
    "\n",
    "  rewards = data.reward * reward_scaling\n",
    "  truncation = data.extras['state_extras']['truncation']\n",
    "  termination = (1 - data.discount) * (1 - truncation)\n",
    "\n",
    "  target_action_log_probs = parametric_action_distribution.log_prob(\n",
    "      policy_logits, data.extras['policy_extras']['raw_action']\n",
    "  )\n",
    "  behaviour_action_log_probs = data.extras['policy_extras']['log_prob']\n",
    "\n",
    "  vs, advantages = compute_gae(\n",
    "      truncation=truncation,\n",
    "      termination=termination,\n",
    "      rewards=rewards,\n",
    "      values=baseline,\n",
    "      bootstrap_value=bootstrap_value,\n",
    "      lambda_=gae_lambda,\n",
    "      discount=discounting,\n",
    "  )\n",
    "  if normalize_advantage:\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "  \n",
    "  # TODO: Compute probability ratio between new and old policy\n",
    "  # Hint: Use jnp.exp() and compute the difference between log probabilities\n",
    "  rho_s = jnp.exp(target_action_log_probs - behaviour_action_log_probs)\n",
    "  \n",
    "  # TODO: Implement the PPO clipped surrogate objective\n",
    "  # Hint: Compare unclipped (rho_s * advantages) vs clipped version\n",
    "  # Use jnp.clip() with clipping_epsilon parameter\n",
    "  surrogate_loss1 = rho_s * advantages\n",
    "  surrogate_loss2 = jnp.clip(rho_s, 1 - clipping_epsilon, 1 + clipping_epsilon) * advantages\n",
    "\n",
    "  # TODO: PPO policy loss is the negative mean of the minimum of both surrogates\n",
    "  policy_loss = -jnp.mean(jnp.minimum(surrogate_loss1, surrogate_loss2))\n",
    "\n",
    "  # TODO: Implement value function loss\n",
    "  # Hint: Mean squared error between vs (targets) and baseline (predictions)\n",
    "  # Scale by 0.5 * 0.5 as in the original implementation\n",
    "  v_error =(vs - baseline)**2\n",
    "  v_loss = 0.25*jnp.mean(v_error) # 0.5 seems to be faster\n",
    "\n",
    "  # TODO: Implement entropy loss for exploration\n",
    "  # Hint: Use parametric_action_distribution.entropy() and multiply by entropy_cost\n",
    "  # Make it negative since we want to maximize entropy (minimize negative entropy)\n",
    "  entropy = parametric_action_distribution.entropy(policy_logits, rng)\n",
    "  entropy_loss = -entropy_cost*jnp.mean(entropy)\n",
    "\n",
    "  # TODO: Combine all loss components\n",
    "  total_loss = policy_loss + v_loss + entropy_loss\n",
    "  \n",
    "  return total_loss, {\n",
    "      'total_loss': total_loss,\n",
    "      'policy_loss': policy_loss,\n",
    "      'v_loss': v_loss,\n",
    "      'entropy_loss': entropy_loss,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef895140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import functools\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground import registry\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from IPython.display import HTML, clear_output\n",
    "import mujoco\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "import cv2\n",
    "import custom_ppo_train\n",
    "from utils import render_video_during_training, evaluate_policy\n",
    "import mediapy as media\n",
    "\n",
    "\n",
    "\n",
    "env_cfg = default_config()\n",
    "\n",
    "x_data, y_data, y_dataerr = [], [], []\n",
    "times = [datetime.now()]\n",
    "\n",
    "# Store the current policy for video rendering\n",
    "current_policy = None\n",
    "\n",
    "num_timesteps = 10\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    times.append(datetime.now())\n",
    "    x_data.append(num_steps)\n",
    "    y_data.append(metrics[\"eval/episode_reward\"])\n",
    "    y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n",
    "\n",
    "    plt.xlim([0, num_timesteps * 1.25])\n",
    "    plt.xlabel(\"# environment steps\")\n",
    "    plt.ylabel(\"reward per episode\")\n",
    "    plt.title(f\"y={y_data[-1]:.3f}\")\n",
    "    plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n",
    "    \n",
    "    # Save the plot to a file instead of displaying it\n",
    "    display(plt.gcf())\n",
    "\n",
    "    # # Render video if we have a current policy\n",
    "    # if current_policy is not None:\n",
    "    #     render_video_during_training(current_policy, num_steps, jit_step, jit_reset, env_cfg, eval_env_for_video)\n",
    "\n",
    "ppo_params = locomotion_params.brax_ppo_config('Go1JoystickRoughTerrain')\n",
    "ppo_training_params = dict(ppo_params)\n",
    "\n",
    "ppo_training_params[\"num_evals\"] = 2 # Reduce for final training for less feedback.\n",
    "ppo_training_params[\"num_timesteps\"] = 10  # Total number of training steps (originally 200000000)\n",
    " \n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "\n",
    "if \"network_factory\" in ppo_params:\n",
    "    del ppo_training_params[\"network_factory\"]\n",
    "    network_factory = functools.partial(\n",
    "        ppo_networks.make_ppo_networks,\n",
    "        **ppo_params.network_factory\n",
    "    )\n",
    "print(ppo_training_params)\n",
    "\n",
    "\n",
    "# Create a policy parameters callback to capture the current policy\n",
    "def policy_params_callback(_, make_policy_fn, params):\n",
    "    # Update the current policy for video rendering\n",
    "    global current_policy\n",
    "    current_policy = make_policy_fn(params, deterministic=True)\n",
    "    \n",
    "train_fn = functools.partial(\n",
    "        custom_ppo_train.train,\n",
    "        **ppo_training_params,\n",
    "        network_factory=network_factory,\n",
    "        # randomization_fn=randomizer,\n",
    "        progress_fn=progress,\n",
    "        policy_params_fn=policy_params_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d261bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "make_policy, params, _ = train_fn(environment=env,\n",
    "                                  eval_env=env,\n",
    "                                  wrap_env_fn=wrapper.wrap_for_brax_training,\n",
    "                                  compute_custom_ppo_loss_fn=compute_custom_ppo_loss\n",
    "                                 )\n",
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KTH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
