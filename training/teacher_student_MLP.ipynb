{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher-Student Distillation\n",
    "\n",
    "In the following code we're loading a policy trained from train.ipynb that has access to privileged information and we train a student network that relies solely on the non-privileged state to mimic the behaviour of the teacher.\n",
    "\n",
    "Our approach is an on-policy student learning that steps the environment based on the student actions and is trained by computing how the teacher would behave in the same situation, similar to DAgger (https://imitation.readthedocs.io/en/latest/algorithms/dagger.html, https://arxiv.org/abs/1011.0686)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory (EAI2025_RL_FINAL)\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "# Add it to sys.path\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 19:30:01.702778: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training and visualization script for Go1 with height scanner, including student distillation.\"\"\"\n",
    "import os\n",
    "\n",
    "# Set environment variables BEFORE importing mujoco to ensure headless offscreen rendering\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import functools\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from brax.training.acme import running_statistics\n",
    "import mujoco\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from environments.custom_env import Joystick, default_config\n",
    "from mujoco_playground._src.gait import draw_joystick_command\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import mediapy as media\n",
    "import imageio\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Persistent display handles so video is not cleared by metric updates\n",
    "metrics_handle = None\n",
    "video_handle = None\n",
    "\n",
    "# Environment setup\n",
    "xml_path = '../environments/custom_env.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "# Batched env helpers (parallelize over environments)\n",
    "batched_reset = jax.jit(jax.vmap(env.reset))\n",
    "batched_step = jax.jit(jax.vmap(env.step))\n",
    "\n",
    "env_cfg = default_config()\n",
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [0.0, 0.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2 * jnp.pi]\n",
    "\n",
    "## Training configuration ####################################################################\n",
    "seed = 42\n",
    "num_envs = 128  # Parallel environments for batched rollout\n",
    "episode_length = 400\n",
    "action_repeat = 1\n",
    "episodes = 1000\n",
    "batch_size = 64\n",
    "batches = (episode_length * num_envs) // batch_size\n",
    "learning_rate = 1e-4\n",
    "data_collection = \"dagger\" # \"bc\" or \"dagger\"\n",
    "loss_function_name = \"mse\" # \"mse\" or \"kl\"\n",
    "student_observation_key = \"state\" # \" \"student_state\" or \"state\" or \"privileged_state\"\n",
    "experiment_name = f\"student_{data_collection}_{loss_function_name}_{student_observation_key}\"\n",
    "assert(data_collection in [\"bc\", \"dagger\"])\n",
    "assert(loss_function_name in [\"mse\", \"kl\"] )\n",
    "##############################################################################################\n",
    "\n",
    "# Folder setup\n",
    "experiment_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'results', experiment_name))\n",
    "os.makedirs(experiment_path, exist_ok=True)\n",
    "\n",
    "# Discover observation/action shapes from the real env\n",
    "_dummy_key = jax.random.PRNGKey(0)\n",
    "dummy_state = jit_reset(_dummy_key)\n",
    "obs_shape = jax.tree_util.tree_map(lambda x: x.shape, dummy_state.obs)\n",
    "action_size = env.action_size\n",
    "student_obs_dim = int(dummy_state.obs[student_observation_key].shape[0])\n",
    "\n",
    "# Teacher network setup (load params saved by train.ipynb)\n",
    "_loaded = np.load(\"../parameters/params_with_height_and_knee.npy\", allow_pickle=True)\n",
    "# np.save of a tuple can load either as 0-d object array or (3,) object array\n",
    "if getattr(_loaded, 'ndim', 1) == 0:\n",
    "    normalizer_params, policy_params, value_params = _loaded.item()\n",
    "else:\n",
    "    normalizer_params, policy_params, value_params = tuple(_loaded.tolist())\n",
    "teacher_params = (normalizer_params, policy_params, value_params)\n",
    "\n",
    "normalize = running_statistics.normalize\n",
    "ppo_params = locomotion_params.brax_ppo_config('Go1JoystickRoughTerrain')\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "if hasattr(ppo_params, 'network_factory'):\n",
    "    network_factory = functools.partial(ppo_networks.make_ppo_networks, **ppo_params.network_factory)\n",
    "\n",
    "# Build teacher network with the SAME observation structure used at training time\n",
    "ppo_network = network_factory(obs_shape, action_size, preprocess_observations_fn=normalize)\n",
    "make_policy = ppo_networks.make_inference_fn(ppo_network)\n",
    "# Teacher inference expects full observation tree (state.obs)\n",
    "teacher_policy_fn = jax.jit(make_policy(teacher_params, deterministic=True))\n",
    "batched_teacher_policy = jax.jit(jax.vmap(lambda obs, rng: teacher_policy_fn(obs, rng)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 19:30:24.848088: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 19:30:26.440388: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student network initialized! Input shape: (64, 52), Output shape: (64, 24)\n"
     ]
    }
   ],
   "source": [
    "# Student network definition\n",
    "class StudentPolicy(nn.Module):\n",
    "    action_size: int\n",
    "    hidden_size: int = 256\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=(self.hidden_size//2))(x)\n",
    "        x = nn.relu(x)\n",
    "        logits = nn.Dense(features=2 * self.action_size)(x)  # Means and log_stds\n",
    "        return logits\n",
    "\n",
    "# Initialize student network\n",
    "student_net = StudentPolicy(action_size=action_size)\n",
    "\n",
    "dummy_input = jnp.ones((batch_size, student_obs_dim))\n",
    "key_student = jax.random.PRNGKey(42)\n",
    "student_params = student_net.init(key_student, dummy_input)\n",
    "optimizer = optax.adamw(learning_rate)\n",
    "opt_state = optimizer.init(student_params)\n",
    "print(f\"Student network initialized! Input shape: {dummy_input.shape}, Output shape: {(batch_size, 2 * action_size)}\")\n",
    "\n",
    "# Vectorized apply for student (maps across leading batch dim)\n",
    "apply_student_batched = jax.jit(jax.vmap(lambda p, x: student_net.apply(p, x)))\n",
    "\n",
    "# Evaluation function\n",
    "# policy_fn should be a function: (obs_batch: (B, student_obs_dim), rng) -> logits (B, 2*action_size)\n",
    "def evaluate_policy(env, policy_fn, key, steps=episode_length, B=1):\n",
    "    # Reset B envs; default B=1 for eval\n",
    "    keys = jax.random.split(key, B)\n",
    "    states = jax.vmap(jit_reset)(keys) if B > 1 else jit_reset(key)\n",
    "    total_reward = 0.0\n",
    "    for _ in range(steps):\n",
    "        if B > 1:\n",
    "            key_step = jax.random.split(key, 1)[0]\n",
    "            obs = states.obs[student_observation_key]  # (B, student_obs_dim)\n",
    "            logits = policy_fn(obs, key_step)\n",
    "            mu = logits[:, :action_size]\n",
    "            actions = jnp.tanh(mu)\n",
    "            states = jax.vmap(jit_step)(states, actions)\n",
    "            total_reward += jnp.sum(states.reward)\n",
    "        else:\n",
    "            key, act_key = jax.random.split(key)\n",
    "            obs = states.obs[student_observation_key].reshape(1, -1)\n",
    "            logits = policy_fn(obs, act_key)\n",
    "            mu = logits[0, :action_size]\n",
    "            actions = jnp.tanh(mu)\n",
    "            states = jit_step(states, actions)\n",
    "            total_reward += states.reward\n",
    "    return float(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 19:31:00.555517: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-10-15 19:31:00.555661: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-10-15 19:31:01.695389: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-10-15 19:31:59.146262: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-10-15 19:31:59.146311: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-10-15 19:31:59.146327: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-10-15 19:31:59.146346: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-10-15 19:31:59.146357: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/jax/_src/api_util.py:53\u001b[0m, in \u001b[0;36m_ensure_index_tuple\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m   \u001b[39mreturn\u001b[39;00m (operator\u001b[39m.\u001b[39;49mindex(x),)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m student_targets \u001b[39m=\u001b[39m student_targets\u001b[39m.\u001b[39mat[step]\u001b[39m.\u001b[39mset(logits)\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m \u001b[39mif\u001b[39;00m data_collection \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdagger\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     \u001b[39m# DAgger: stepping on the Student action\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     student_logits \u001b[39m=\u001b[39m student_net\u001b[39m.\u001b[39;49mapply(student_params, obs_batch)  \u001b[39m# (N, 2*A)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m     mu \u001b[39m=\u001b[39m student_logits[:, :action_size]\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m     actions \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mtanh(mu)  \u001b[39m# (N, A)\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/flax/linen/module.py:2241\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, variables, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m   method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m\n\u001b[1;32m   2240\u001b[0m method \u001b[39m=\u001b[39m _get_unbound_fn(method)\n\u001b[0;32m-> 2241\u001b[0m \u001b[39mreturn\u001b[39;00m apply(\n\u001b[1;32m   2242\u001b[0m   method,\n\u001b[1;32m   2243\u001b[0m   \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2244\u001b[0m   mutable\u001b[39m=\u001b[39;49mmutable,\n\u001b[1;32m   2245\u001b[0m   capture_intermediates\u001b[39m=\u001b[39;49mcapture_intermediates,\n\u001b[1;32m   2246\u001b[0m )(variables, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, rngs\u001b[39m=\u001b[39;49mrngs)\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/flax/core/scope.py:1079\u001b[0m, in \u001b[0;36mapply.<locals>.wrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m   \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mApplyScopeInvalidVariablesStructureError(variables)\n\u001b[1;32m   1076\u001b[0m \u001b[39mwith\u001b[39;00m bind(\n\u001b[1;32m   1077\u001b[0m   variables, rngs\u001b[39m=\u001b[39mrngs, mutable\u001b[39m=\u001b[39mmutable, flags\u001b[39m=\u001b[39mflags\n\u001b[1;32m   1078\u001b[0m )\u001b[39m.\u001b[39mtemporary() \u001b[39mas\u001b[39;00m root:\n\u001b[0;32m-> 1079\u001b[0m   y \u001b[39m=\u001b[39m fn(root, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1080\u001b[0m \u001b[39mif\u001b[39;00m mutable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   1081\u001b[0m   \u001b[39mreturn\u001b[39;00m y, root\u001b[39m.\u001b[39mmutable_variables()\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/flax/linen/module.py:3023\u001b[0m, in \u001b[0;36mapply.<locals>.scope_fn\u001b[0;34m(scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m _context\u001b[39m.\u001b[39mcapture_stack\u001b[39m.\u001b[39mappend(capture_intermediates)\n\u001b[1;32m   3022\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3023\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(module\u001b[39m.\u001b[39;49mclone(parent\u001b[39m=\u001b[39;49mscope, _deep_clone\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3024\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m   _context\u001b[39m.\u001b[39mcapture_stack\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/flax/linen/module.py:699\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    698\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 699\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    700\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/flax/linen/module.py:1216\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1215\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m-> 1216\u001b[0m     y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m   y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32m/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDense(features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)(x)\n\u001b[1;32m      <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m----> <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mDense(features\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m))(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://gpu1.eecs.kth.se/home/jovyan/EAI2025_RL_FINAL/training/teacher_student_MLP.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m logits \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDense(features\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_size)(x)  \u001b[39m# Means and log_stds\u001b[39;00m\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/flax/linen/module.py:699\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    698\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 699\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    700\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/flax/linen/module.py:1216\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m   1215\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m-> 1216\u001b[0m     y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1218\u001b[0m   y \u001b[39m=\u001b[39m run_fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/flax/linen/linear.py:288\u001b[0m, in \u001b[0;36mDense.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    287\u001b[0m   dot_general \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mdot_general\n\u001b[0;32m--> 288\u001b[0m y \u001b[39m=\u001b[39m dot_general(\n\u001b[1;32m    289\u001b[0m   inputs,\n\u001b[1;32m    290\u001b[0m   kernel,\n\u001b[1;32m    291\u001b[0m   (((inputs\u001b[39m.\u001b[39;49mndim \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m,), (\u001b[39m0\u001b[39;49m,)), ((), ())),\n\u001b[1;32m    292\u001b[0m   precision\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision,\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m bias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m   y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mreshape(bias, (\u001b[39m1\u001b[39m,) \u001b[39m*\u001b[39m (y\u001b[39m.\u001b[39mndim \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/jax/_src/lax/lax.py:2521\u001b[0m, in \u001b[0;36mdot_general\u001b[0;34m(lhs, rhs, dimension_numbers, precision, preferred_element_type, out_sharding)\u001b[0m\n\u001b[1;32m   2517\u001b[0m (lhs_contract, rhs_contract), (lhs_batch, rhs_batch) \u001b[39m=\u001b[39m dimension_numbers\n\u001b[1;32m   2518\u001b[0m cdims \u001b[39m=\u001b[39m (api_util\u001b[39m.\u001b[39m_ensure_index_tuple(lhs_contract),\n\u001b[1;32m   2519\u001b[0m          api_util\u001b[39m.\u001b[39m_ensure_index_tuple(rhs_contract))\n\u001b[1;32m   2520\u001b[0m bdims \u001b[39m=\u001b[39m (api_util\u001b[39m.\u001b[39m_ensure_index_tuple(lhs_batch),\n\u001b[0;32m-> 2521\u001b[0m          api_util\u001b[39m.\u001b[39;49m_ensure_index_tuple(rhs_batch))\n\u001b[1;32m   2522\u001b[0m preferred_element_type \u001b[39m=\u001b[39m (\n\u001b[1;32m   2523\u001b[0m     \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m preferred_element_type \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m\n\u001b[1;32m   2524\u001b[0m     dtypes\u001b[39m.\u001b[39mcanonicalize_dtype(np\u001b[39m.\u001b[39mdtype(preferred_element_type)))\n\u001b[1;32m   2525\u001b[0m lhs, rhs \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mstandard_insert_pvary(lhs, rhs)\n",
      "File \u001b[0;32m~/EAI2025_RL_FINAL/venv_rl/lib/python3.10/site-packages/jax/_src/api_util.py:53\u001b[0m, in \u001b[0;36m_ensure_index_tuple\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     51\u001b[0m x \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39mconcrete_or_error(\u001b[39mNone\u001b[39;00m, x, \u001b[39m\"\u001b[39m\u001b[39mexpected a static index or sequence of indices.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m   \u001b[39mreturn\u001b[39;00m (operator\u001b[39m.\u001b[39;49mindex(x),)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(\u001b[39mmap\u001b[39m(operator\u001b[39m.\u001b[39mindex, x))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to get teacher logits\n",
    "@jax.jit\n",
    "def get_teacher_logits(observations):\n",
    "    # observations can be single or batched observation pytree\n",
    "    param_subset = (teacher_params[0], teacher_params[1])\n",
    "    return ppo_network.policy_network.apply(*param_subset, observations)\n",
    "\n",
    "# Training function with MSE loss\n",
    "def kl_train_step(params, opt_state, inputs, targets):\n",
    "    def loss_fn(params):\n",
    "        # Student predictions: (batch_size, 2 * action_size) -> (mu, raw_log_std)\n",
    "        predictions = student_net.apply(params, inputs)  # (batch_size, 2 * action_size)\n",
    "        mu_s = predictions[:, :action_size]  # (batch_size, action_size)\n",
    "        raw_log_std_s = predictions[:, action_size:]  # (batch_size, action_size)\n",
    "        std_s = jax.nn.softplus(raw_log_std_s) + 1e-8  # Ensure positive std\n",
    "\n",
    "        # Teacher targets: (batch_size, 2 * action_size) -> (loc, scale)\n",
    "        loc_t = targets[:, :action_size]  # (batch_size, action_size)\n",
    "        scale_t = targets[:, action_size:]  # (batch_size, action_size)\n",
    "\n",
    "        # KL divergence for Normal distributions (pre-tanh)\n",
    "        # KL(N(mu1, sigma1) || N(mu2, sigma2)) = 0.5 * (log(sigma2/sigma1) + (sigma1^2 + (mu1-mu2)^2)/(2*sigma2^2) - 0.5)\n",
    "        log_std_s = jnp.log(std_s + 1e-8)\n",
    "        log_std_t = jnp.log(scale_t + 1e-8)\n",
    "        kl_term = (\n",
    "            log_std_t - log_std_s +\n",
    "            (std_s**2 + (mu_s - loc_t)**2) / (2 * scale_t**2 + 1e-8) -\n",
    "            0.5\n",
    "        )\n",
    "        loss = jnp.mean(kl_term)  # Mean over batch and action dims\n",
    "        return loss\n",
    "\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Training function with MSE loss\n",
    "@jax.jit\n",
    "def mse_train_step(params, opt_state, inputs, targets):\n",
    "    def loss_fn(params):\n",
    "        predictions = student_net.apply(params, inputs)\n",
    "        loss = jnp.mean((predictions - targets) ** 2)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "training_losses = []\n",
    "eval_rewards = []\n",
    "eval_episodes = []\n",
    "metrics_handle = None\n",
    "video_handle = None\n",
    "\n",
    "if loss_function_name == \"mse\":\n",
    "    train_step = mse_train_step\n",
    "elif loss_function_name == \"kl\":\n",
    "    train_step = kl_train_step\n",
    "else:\n",
    "    print(\"LOSS Function NOT IMPLEMENTED\")\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "for episode in range(episodes):\n",
    "    key, env_key, act_key = jax.random.split(key, 3)\n",
    "    # Reset num_envs in parallel\n",
    "    env_keys = jax.random.split(env_key, num_envs)\n",
    "    states = batched_reset(env_keys)\n",
    "\n",
    "    # Generate a different random command for each environment: x~U(0,1), y~U(0,1), z=0\n",
    "    act_key, cmd_key = jax.random.split(act_key)\n",
    "    rand_xyz = jax.random.uniform(cmd_key, shape=(num_envs, 3), minval=0.0, maxval=1.0)\n",
    "    # zeros_z = jnp.zeros((num_envs, 1))\n",
    "    # command_batch = jnp.concatenate([rand_xy, zeros_z], axis=-1)  # (N, 3)\n",
    "    command_batch = rand_xyz\n",
    "    states.info[\"command\"] = command_batch\n",
    "\n",
    "    # Buffers: (T, N, ...)\n",
    "    student_inputs = jnp.zeros((episode_length, num_envs, student_obs_dim))\n",
    "    student_targets = jnp.zeros((episode_length, num_envs, 2 * action_size))\n",
    "\n",
    "    for step in range(episode_length):\n",
    "        # Teacher logits target on current states (batched)\n",
    "        logits = get_teacher_logits(states.obs)  # (N, 2*A)\n",
    "\n",
    "        obs_batch = states.obs[student_observation_key]  # (N, student_obs_dim)\n",
    "        student_inputs = student_inputs.at[step].set(obs_batch)\n",
    "        student_targets = student_targets.at[step].set(logits)\n",
    "        \n",
    "        if data_collection == \"dagger\":\n",
    "            # DAgger: stepping on the Student action\n",
    "            student_logits = student_net.apply(student_params, obs_batch)  # (N, 2*A)\n",
    "            mu = student_logits[:, :action_size]\n",
    "            actions = jnp.tanh(mu)  # (N, A)\n",
    "        else:\n",
    "            # Behaviour Cloning: stepping on the Teacher action\n",
    "            act_rngs = jax.random.split(act_key, num_envs)  # Split RNG per env\n",
    "            actions = batched_teacher_policy(states.obs, act_rngs)  # (N, A)\n",
    "        states = batched_step(states, actions)\n",
    "        states.info[\"command\"] = command_batch\n",
    "\n",
    "    # Flatten buffers to (T*N, ...)\n",
    "    flat_inputs = student_inputs.reshape((-1, student_obs_dim))\n",
    "    flat_targets = student_targets.reshape((-1, 2 * action_size))\n",
    "\n",
    "    # Mini-batch SGD over collected data; use only full-sized minibatches to avoid recompilation\n",
    "    total_samples = flat_inputs.shape[0]\n",
    "    total_full = (total_samples // batch_size) * batch_size\n",
    "    num_minibatches = max(1, total_full // batch_size)\n",
    "    total_loss = 0.0\n",
    "    for b in range(num_minibatches):\n",
    "        start_idx = b * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_in = flat_inputs[start_idx:end_idx]\n",
    "        batch_tg = flat_targets[start_idx:end_idx]\n",
    "        student_params, opt_state, loss = train_step(student_params, opt_state, batch_in, batch_tg)\n",
    "        total_loss += loss\n",
    "    avg_loss = total_loss / num_minibatches\n",
    "    training_losses.append(float(avg_loss))\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        # Student inference function: outputs logits for a batch; eval with B=1 for simplicity\n",
    "        student_policy_fn = jax.jit(lambda obs, rng: student_net.apply(student_params, obs))\n",
    "        eval_reward = evaluate_policy(env, student_policy_fn, act_key, steps=episode_length, B=1)\n",
    "        eval_rewards.append(float(eval_reward))\n",
    "        eval_episodes.append(episode + 1)\n",
    "\n",
    "        global metrics_handle\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        axes[0].plot(range(1, len(training_losses) + 1), [float(x) for x in training_losses], label=\"Training Loss\")\n",
    "        axes[0].set_xlabel(\"Episode\")\n",
    "        axes[0].set_ylabel(\"MSE Loss\")\n",
    "        axes[0].legend()\n",
    "        axes[0].set_title(\"Training Loss\")\n",
    "        axes[1].plot(eval_episodes, eval_rewards, label=\"Eval Reward\")\n",
    "        axes[1].set_xlabel(\"Episode\")\n",
    "        axes[1].set_ylabel(\"Eval Reward\")\n",
    "        axes[1].legend()\n",
    "        axes[1].set_title(\"Evaluation Reward\")\n",
    "        plt.tight_layout()\n",
    "        if metrics_handle is None:\n",
    "            metrics_handle = display(fig, display_id=True)\n",
    "        else:\n",
    "            metrics_handle.update(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Compute GIFs of the student performance\n",
    "    if (episode + 1) % 1000 == 0:\n",
    "        env_cfg.pert_config.enable = True\n",
    "        env_cfg.pert_config.velocity_kick = [3.0, 6.0]\n",
    "        env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "        env_cfg.command_config.a = [1.5, 0.8, 2*jnp.pi]\n",
    "        eval_env = env\n",
    "        velocity_kick_range = [0.0, 0.0]  # Disable velocity kick.\n",
    "        kick_duration_range = [0.05, 0.2]\n",
    "\n",
    "        jit_reset = jax.jit(eval_env.reset)\n",
    "        jit_step = jax.jit(eval_env.step)\n",
    "        student_policy_fn = jax.jit(lambda obs, rng: student_net.apply(student_params, obs))\n",
    "\n",
    "        # Save policy parameters\n",
    "        params_filename = os.path.join(experiment_path, f\"student_params_{data_collection}_{loss_function_name}.npy\")\n",
    "        params_np = jax.tree.map(lambda x: np.array(x), student_params)\n",
    "        np.save(params_filename, params_np, allow_pickle=True)\n",
    "        # Loading back: to test if this works\n",
    "        # loaded = np.load(params_filename, allow_pickle=True).item()\n",
    "        # loaded_params = jax.tree.map(lambda x: jnp.array(x), loaded)\n",
    "        # student_params = loaded_params\n",
    "\n",
    "        # GIFs Evaluation\n",
    "        x_vels = [0.0, 0.5, 1.0, 0.0, 0.0, 0.5]\n",
    "        y_vels = [0.0, 0.0, 0.0, 0.5, 1.0, 0.5]\n",
    "        yaw_vels = [1.0, 0.0, 0.0, 0.0, 0.0, 0.1]\n",
    "\n",
    "        for run_id, (x_vel, y_vel, yaw_vel) in enumerate(zip(x_vels, y_vels, yaw_vels)):\n",
    "            def sample_pert(rng):\n",
    "                rng, key1, key2 = jax.random.split(rng, 3)\n",
    "                pert_mag = jax.random.uniform(\n",
    "                    key1, minval=velocity_kick_range[0], maxval=velocity_kick_range[1]\n",
    "                )\n",
    "                duration_seconds = jax.random.uniform(\n",
    "                    key2, minval=kick_duration_range[0], maxval=kick_duration_range[1]\n",
    "                )\n",
    "                duration_steps = jnp.round(duration_seconds / eval_env.dt).astype(jnp.int32)\n",
    "                state.info[\"pert_mag\"] = pert_mag\n",
    "                state.info[\"pert_duration\"] = duration_steps\n",
    "                state.info[\"pert_duration_seconds\"] = duration_seconds\n",
    "                return rng\n",
    "\n",
    "\n",
    "            rng = jax.random.PRNGKey(0)\n",
    "            rollout = []\n",
    "            modify_scene_fns = []\n",
    "\n",
    "            swing_peak = []\n",
    "            rewards = []\n",
    "            linvel = []\n",
    "            angvel = []\n",
    "            track = []\n",
    "            foot_vel = []\n",
    "            rews = []\n",
    "            contact = []\n",
    "            command = jnp.array([x_vel, y_vel, yaw_vel])\n",
    "\n",
    "            state = jit_reset(rng)\n",
    "            if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
    "                rng = sample_pert(rng)\n",
    "            state.info[\"command\"] = command\n",
    "            for _ in range(env_cfg.episode_length):\n",
    "                if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
    "                    rng = sample_pert(rng)\n",
    "                act_rng, rng = jax.random.split(rng)\n",
    "                student_obs = state.obs[student_observation_key].reshape(1, -1)\n",
    "                student_logits = student_policy_fn(student_obs, rng)\n",
    "                mu = student_logits[0, :env.action_size]\n",
    "                ctrl_student = jnp.tanh(mu)\n",
    "                state = jit_step(state, ctrl_student)\n",
    "                state.info[\"command\"] = command\n",
    "                rews.append(\n",
    "                    {k: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "                )\n",
    "                rollout.append(state)\n",
    "                swing_peak.append(state.info[\"swing_peak\"])\n",
    "                rewards.append(\n",
    "                    {k[7:]: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "                )\n",
    "                linvel.append(env.get_global_linvel(state.data))\n",
    "                angvel.append(env.get_gyro(state.data))\n",
    "                track.append(\n",
    "                    env._reward_tracking_lin_vel(\n",
    "                        state.info[\"command\"], env.get_local_linvel(state.data)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                feet_vel = state.data.sensordata[env._foot_linvel_sensor_adr]\n",
    "                vel_xy = feet_vel[..., :2]\n",
    "                vel_norm = jnp.sqrt(jnp.linalg.norm(vel_xy, axis=-1))\n",
    "                foot_vel.append(vel_norm)\n",
    "\n",
    "                contact.append(state.info[\"last_contact\"])\n",
    "\n",
    "                xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "                xyz += np.array([0, 0, 0.2])\n",
    "                x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "                yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "                modify_scene_fns.append(\n",
    "                    functools.partial(\n",
    "                        draw_joystick_command,\n",
    "                        cmd=state.info[\"command\"],\n",
    "                        xyz=xyz,\n",
    "                        theta=yaw,\n",
    "                        scl=abs(state.info[\"command\"][0])\n",
    "                        / env_cfg.command_config.a[0],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "            render_every = 2\n",
    "            fps = 1.0 / eval_env.dt / render_every\n",
    "            traj = rollout[::render_every]\n",
    "            mod_fns = modify_scene_fns[::render_every]\n",
    "\n",
    "            scene_option = mujoco.MjvOption()\n",
    "            scene_option.geomgroup[2] = True\n",
    "            scene_option.geomgroup[3] = False\n",
    "            scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "            scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "            scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "            frames = eval_env.render(\n",
    "                traj,\n",
    "                camera=\"track\",\n",
    "                scene_option=scene_option,\n",
    "                width=640,\n",
    "                height=480,\n",
    "                modify_scene_fns=mod_fns,\n",
    "            )\n",
    "            media.show_video(frames, fps=fps)\n",
    "\n",
    "            i_gif_name = os.path.join(experiment_path, f\"teacher_student_{run_id}.gif\")\n",
    "            imageio.mimsave(i_gif_name, frames, duration=1.0 / fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, from utils.py, changing how we compute the action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [3.0, 6.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2*jnp.pi]\n",
    "eval_env = env\n",
    "velocity_kick_range = [0.0, 0.0]  # Disable velocity kick.\n",
    "kick_duration_range = [0.05, 0.2]\n",
    "\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "student_policy_fn = jax.jit(lambda obs, rng: student_net.apply(student_params, obs))\n",
    "\n",
    "x_vels = [0.0, 0.5, 1.0, 0.0, 0.0, 0.5]\n",
    "y_vels = [0.0, 0.0, 0.0, 0.5, 1.0, 0.5]\n",
    "yaw_vels = [1.0, 0.0, 0.0, 0.0, 0.0, 0.1]\n",
    "\n",
    "for run_id, (x_vel, y_vel, yaw_vel) in enumerate(zip(x_vels, y_vels, yaw_vels)):\n",
    "    def sample_pert(rng):\n",
    "        rng, key1, key2 = jax.random.split(rng, 3)\n",
    "        pert_mag = jax.random.uniform(\n",
    "            key1, minval=velocity_kick_range[0], maxval=velocity_kick_range[1]\n",
    "        )\n",
    "        duration_seconds = jax.random.uniform(\n",
    "            key2, minval=kick_duration_range[0], maxval=kick_duration_range[1]\n",
    "        )\n",
    "        duration_steps = jnp.round(duration_seconds / eval_env.dt).astype(jnp.int32)\n",
    "        state.info[\"pert_mag\"] = pert_mag\n",
    "        state.info[\"pert_duration\"] = duration_steps\n",
    "        state.info[\"pert_duration_seconds\"] = duration_seconds\n",
    "        return rng\n",
    "\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    rollout = []\n",
    "    modify_scene_fns = []\n",
    "\n",
    "    swing_peak = []\n",
    "    rewards = []\n",
    "    linvel = []\n",
    "    angvel = []\n",
    "    track = []\n",
    "    foot_vel = []\n",
    "    rews = []\n",
    "    contact = []\n",
    "    command = jnp.array([x_vel, y_vel, yaw_vel])\n",
    "\n",
    "    state = jit_reset(rng)\n",
    "    if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
    "        rng = sample_pert(rng)\n",
    "    state.info[\"command\"] = command\n",
    "    for _ in range(env_cfg.episode_length):\n",
    "        if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
    "            rng = sample_pert(rng)\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        student_obs = state.obs[student_observation_key].reshape(1, -1)\n",
    "        student_logits = student_policy_fn(student_obs, rng)\n",
    "        mu = student_logits[0, :env.action_size]\n",
    "        ctrl_student = jnp.tanh(mu)\n",
    "        state = jit_step(state, ctrl_student)\n",
    "        state.info[\"command\"] = command\n",
    "        rews.append(\n",
    "            {k: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "        )\n",
    "        rollout.append(state)\n",
    "        swing_peak.append(state.info[\"swing_peak\"])\n",
    "        rewards.append(\n",
    "            {k[7:]: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "        )\n",
    "        linvel.append(env.get_global_linvel(state.data))\n",
    "        angvel.append(env.get_gyro(state.data))\n",
    "        track.append(\n",
    "            env._reward_tracking_lin_vel(\n",
    "                state.info[\"command\"], env.get_local_linvel(state.data)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        feet_vel = state.data.sensordata[env._foot_linvel_sensor_adr]\n",
    "        vel_xy = feet_vel[..., :2]\n",
    "        vel_norm = jnp.sqrt(jnp.linalg.norm(vel_xy, axis=-1))\n",
    "        foot_vel.append(vel_norm)\n",
    "\n",
    "        contact.append(state.info[\"last_contact\"])\n",
    "\n",
    "        xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "        xyz += np.array([0, 0, 0.2])\n",
    "        x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "        yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "        modify_scene_fns.append(\n",
    "            functools.partial(\n",
    "                draw_joystick_command,\n",
    "                cmd=state.info[\"command\"],\n",
    "                xyz=xyz,\n",
    "                theta=yaw,\n",
    "                scl=abs(state.info[\"command\"][0])\n",
    "                / env_cfg.command_config.a[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    render_every = 2\n",
    "    fps = 1.0 / eval_env.dt / render_every\n",
    "    traj = rollout[::render_every]\n",
    "    mod_fns = modify_scene_fns[::render_every]\n",
    "\n",
    "    scene_option = mujoco.MjvOption()\n",
    "    scene_option.geomgroup[2] = True\n",
    "    scene_option.geomgroup[3] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "    frames = eval_env.render(\n",
    "        traj,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=640,\n",
    "        height=480,\n",
    "        modify_scene_fns=mod_fns,\n",
    "    )\n",
    "    media.show_video(frames, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison visualization function\n",
    "# Uses a persistent display handle so the video remains visible until replaced\n",
    "\n",
    "def compare_teacher_student_gifs(\n",
    "    env,\n",
    "    jit_reset,\n",
    "    jit_step,\n",
    "    teacher_policy_fn,\n",
    "    student_policy_fn,\n",
    "    student_params,\n",
    "    episode_length,\n",
    "    command,\n",
    "    seed,\n",
    "    width=640,\n",
    "    height=480,\n",
    "    fps=30,\n",
    "    render_every=2,\n",
    "):\n",
    "    global video_handle\n",
    "    scene_option = mujoco.MjvOption()\n",
    "    scene_option.geomgroup[2] = True\n",
    "    scene_option.geomgroup[3] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key_teacher, key_student, key_env = jax.random.split(key, 3)\n",
    "    state_teacher = jit_reset(key_env)\n",
    "    state_student = jit_reset(key_env)\n",
    "    state_teacher.info[\"command\"] = command\n",
    "    state_student.info[\"command\"] = command\n",
    "    rollout_teacher = []\n",
    "    rollout_student = []\n",
    "    modify_scene_fns_teacher = []\n",
    "    modify_scene_fns_student = []\n",
    "    for _ in range(episode_length):\n",
    "        # Teacher: expects full observation tree\n",
    "        act_rng_teacher, key_teacher = jax.random.split(key_teacher)\n",
    "        ctrl_teacher, _ = teacher_policy_fn(state_teacher.obs, act_rng_teacher)\n",
    "        state_teacher = jit_step(state_teacher, ctrl_teacher)\n",
    "        state_teacher.info[\"command\"] = command\n",
    "        rollout_teacher.append(state_teacher)\n",
    "        # Student: logits -> tanh(mu) actions\n",
    "        act_rng_student, key_student = jax.random.split(key_student)\n",
    "        student_obs = state_student.obs[student_observation_key].reshape(1, -1)\n",
    "        student_logits = student_policy_fn(student_obs, act_rng_student)\n",
    "        mu = student_logits[0, :env.action_size]\n",
    "        ctrl_student = jnp.tanh(mu)\n",
    "        state_student = jit_step(state_student, ctrl_student)\n",
    "        state_student.info[\"command\"] = command\n",
    "        rollout_student.append(state_student)\n",
    "        for st, mod_list in [\n",
    "            (state_teacher, modify_scene_fns_teacher),\n",
    "            (state_student, modify_scene_fns_student),\n",
    "        ]:\n",
    "            xyz = np.array(st.data.xpos[env._torso_body_id])\n",
    "            xyz += np.array([0, 0, 0.2])\n",
    "            x_axis = st.data.xmat[env._torso_body_id, 0]\n",
    "            yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "            mod_list.append(\n",
    "                functools.partial(\n",
    "                    draw_joystick_command,\n",
    "                    cmd=st.info[\"command\"],\n",
    "                    xyz=xyz,\n",
    "                    theta=yaw,\n",
    "                    scl=abs(st.info[\"command\"][0]) / env_cfg.command_config.a[0],\n",
    "                )\n",
    "            )\n",
    "    traj_teacher = rollout_teacher[::render_every]\n",
    "    traj_student = rollout_student[::render_every]\n",
    "    mod_fns_teacher = modify_scene_fns_teacher[::render_every]\n",
    "    mod_fns_student = modify_scene_fns_student[::render_every]\n",
    "    # Use env.render which is implemented to render offscreen with EGL\n",
    "    frames_teacher = env.render(\n",
    "        traj_teacher,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        modify_scene_fns=mod_fns_teacher,\n",
    "    )\n",
    "    frames_student = env.render(\n",
    "        traj_student,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        modify_scene_fns=mod_fns_student,\n",
    "    )\n",
    "    teacher_gif_path = \"teacher_policy.gif\"\n",
    "    student_gif_path = \"student_policy.gif\"\n",
    "    # Ensure frames are uint8\n",
    "    frames_teacher = [np.asarray(f, dtype=np.uint8) for f in frames_teacher]\n",
    "    frames_student = [np.asarray(f, dtype=np.uint8) for f in frames_student]\n",
    "    imageio.mimsave(teacher_gif_path, frames_teacher, fps=fps, loop=0)\n",
    "    imageio.mimsave(student_gif_path, frames_student, fps=fps, loop=0)\n",
    "    def gif_to_base64(gif_path):\n",
    "        with open(gif_path, \"rb\") as f:\n",
    "            encoded = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "        return f\"data:image/gif;base64,{encoded}\"\n",
    "    teacher_base64 = gif_to_base64(teacher_gif_path)\n",
    "    student_base64 = gif_to_base64(student_gif_path)\n",
    "    html = f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: center;\">\n",
    "        <div style=\"margin-right: 10px; text-align: center;\">\n",
    "            <h3>Teacher Policy</h3>\n",
    "            <img src=\"{teacher_base64}\" width=\"{width}\" height=\"{height}\"/>\n",
    "        </div>\n",
    "        <div style=\"text-align: center;\">\n",
    "            <h3>Student Policy</h3>\n",
    "            <img src=\"{student_base64}\" width=\"{width}\" height=\"{height}\"/>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    if video_handle is None:\n",
    "        video_handle = display(HTML(html), display_id=True)\n",
    "    else:\n",
    "        video_handle.update(HTML(html))\n",
    "\n",
    "    os.remove(teacher_gif_path)\n",
    "    os.remove(student_gif_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
