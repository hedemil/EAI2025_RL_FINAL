{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training and visualization script for Go1 with height scanner, including student distillation.\"\"\"\n",
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import functools\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from brax.training.acme import running_statistics\n",
    "import mujoco\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from custom_env import Joystick, default_config\n",
    "from mujoco_playground._src.gait import draw_joystick_command\n",
    "from IPython.display import HTML, display\n",
    "import mediapy as media\n",
    "import imageio\n",
    "import base64\n",
    "\n",
    "# Set environment variables for GPU usage\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "# Environment setup\n",
    "xml_path = 'custom_env.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "\n",
    "env_cfg = default_config()\n",
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [0.0, 0.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2 * jnp.pi]\n",
    "\n",
    "# Training configuration\n",
    "seed = 42\n",
    "num_envs = 1  # Single environment for simplicity\n",
    "episode_length = 128\n",
    "action_repeat = 1\n",
    "episodes = 10\n",
    "batch_size = 32  # Adjusted for multiple batches\n",
    "batches = episode_length // batch_size\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Discover observation/action shapes from the real env\n",
    "_dummy_key = jax.random.PRNGKey(0)\n",
    "dummy_state = jit_reset(_dummy_key)\n",
    "obs_shape = jax.tree_util.tree_map(lambda x: x.shape, dummy_state.obs)\n",
    "action_size = env.action_size\n",
    "student_obs_dim = int(dummy_state.obs['state'].shape[0])\n",
    "\n",
    "# Teacher network setup (load params saved by train.ipynb)\n",
    "_loaded = np.load(\"params.npy\", allow_pickle=True)\n",
    "# np.save of a tuple can load either as 0-d object array or (3,) object array\n",
    "if getattr(_loaded, 'ndim', 1) == 0:\n",
    "    normalizer_params, policy_params, value_params = _loaded.item()\n",
    "else:\n",
    "    normalizer_params, policy_params, value_params = tuple(_loaded.tolist())\n",
    "teacher_params = (normalizer_params, policy_params, value_params)\n",
    "\n",
    "normalize = running_statistics.normalize\n",
    "ppo_params = locomotion_params.brax_ppo_config('Go1JoystickRoughTerrain')\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "if hasattr(ppo_params, 'network_factory'):\n",
    "    network_factory = functools.partial(ppo_networks.make_ppo_networks, **ppo_params.network_factory)\n",
    "\n",
    "# Build teacher network with the SAME observation structure used at training time\n",
    "ppo_network = network_factory(obs_shape, action_size, preprocess_observations_fn=normalize)\n",
    "make_policy = ppo_networks.make_inference_fn(ppo_network)\n",
    "# Teacher inference expects full observation tree (state.obs)\n",
    "jit_inference_fn = jax.jit(make_policy(teacher_params, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student network definition\n",
    "class StudentPolicy(nn.Module):\n",
    "    action_size: int\n",
    "    hidden_size: int = 100\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        logits = nn.Dense(features=2 * self.action_size)(x)  # Means and log_stds\n",
    "        return logits\n",
    "\n",
    "# Initialize student network\n",
    "student_net = StudentPolicy(action_size=action_size)\n",
    "\n",
    "dummy_input = jnp.ones((batch_size, student_obs_dim))\n",
    "key_student = jax.random.PRNGKey(42)\n",
    "student_params = student_net.init(key_student, dummy_input)\n",
    "optimizer = optax.adamw(learning_rate)\n",
    "opt_state = optimizer.init(student_params)\n",
    "print(f\"Student network initialized! Input shape: {dummy_input.shape}, Output shape: {(batch_size, 2 * action_size)}\")\n",
    "\n",
    "# Evaluation function\n",
    "# policy_fn should be a function: (obs_batch: (1, student_obs_dim), rng) -> logits (1, 2*action_size)\n",
    "def evaluate_policy(env, policy_fn, key, steps=episode_length):\n",
    "    state = jit_reset(key)\n",
    "    total_reward = 0.0\n",
    "    for _ in range(steps):\n",
    "        key, act_key = jax.random.split(key)\n",
    "        obs = state.obs['state']  # use non-privileged observations for student\n",
    "        # Add batch dimension for the network\n",
    "        obs_batch = obs.reshape(1, -1)\n",
    "        logits = policy_fn(obs_batch, act_key)\n",
    "        # Convert mean logits to actions via tanh (teacher deterministic action semantics)\n",
    "        mu = logits[0, :action_size]\n",
    "        actions = jnp.tanh(mu)\n",
    "        state = jit_step(state, actions)\n",
    "        total_reward += state.reward\n",
    "    return float(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison visualization function\n",
    "def compare_teacher_student_gifs(\n",
    "    env,\n",
    "    jit_reset,\n",
    "    jit_step,\n",
    "    teacher_policy_fn,\n",
    "    student_policy_fn,\n",
    "    student_params,\n",
    "    episode_length,\n",
    "    command,\n",
    "    seed,\n",
    "    width=640,\n",
    "    height=480,\n",
    "    fps=30,\n",
    "    render_every=2,\n",
    "):\n",
    "    scene_option = mujoco.MjvOption()\n",
    "    scene_option.geomgroup[2] = True\n",
    "    scene_option.geomgroup[3] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key_teacher, key_student, key_env = jax.random.split(key, 3)\n",
    "\n",
    "    state_teacher = jit_reset(key_env)\n",
    "    state_student = jit_reset(key_env)\n",
    "    state_teacher.info[\"command\"] = command\n",
    "    state_student.info[\"command\"] = command\n",
    "\n",
    "    rollout_teacher = []\n",
    "    rollout_student = []\n",
    "    modify_scene_fns_teacher = []\n",
    "    modify_scene_fns_student = []\n",
    "\n",
    "    for step in range(episode_length):\n",
    "        # Teacher: expects full observation tree\n",
    "        act_rng_teacher, key_teacher = jax.random.split(key_teacher)\n",
    "        ctrl_teacher, _ = teacher_policy_fn(state_teacher.obs, act_rng_teacher)\n",
    "        state_teacher = jit_step(state_teacher, ctrl_teacher)\n",
    "        state_teacher.info[\"command\"] = command\n",
    "        rollout_teacher.append(state_teacher)\n",
    "\n",
    "        # Student: takes non-privileged obs normalized similarly\n",
    "        act_rng_student, key_student = jax.random.split(key_student)\n",
    "        student_obs = state_student.obs['state'].reshape(1, -1)\n",
    "        student_logits = student_policy_fn(student_obs, act_rng_student)\n",
    "        mu = student_logits[0, :env.action_size]\n",
    "        ctrl_student = jnp.tanh(mu)\n",
    "        state_student = jit_step(state_student, ctrl_student)\n",
    "        state_student.info[\"command\"] = command\n",
    "        rollout_student.append(state_student)\n",
    "\n",
    "        for state, modify_scene_fns in [\n",
    "            (state_teacher, modify_scene_fns_teacher),\n",
    "            (state_student, modify_scene_fns_student),\n",
    "        ]:\n",
    "            xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "            xyz += np.array([0, 0, 0.2])\n",
    "            x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "            yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "            modify_scene_fns.append(\n",
    "                functools.partial(\n",
    "                    draw_joystick_command,\n",
    "                    cmd=state.info[\"command\"],\n",
    "                    xyz=xyz,\n",
    "                    theta=yaw,\n",
    "                    scl=abs(state.info[\"command\"][0]) / env_cfg.command_config.a[0],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    traj_teacher = rollout_teacher[::render_every]\n",
    "    traj_student = rollout_student[::render_every]\n",
    "    mod_fns_teacher = modify_scene_fns_teacher[::render_every]\n",
    "    mod_fns_student = modify_scene_fns_student[::render_every]\n",
    "\n",
    "    frames_teacher = env.render(\n",
    "        traj_teacher,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        modify_scene_fns=mod_fns_teacher,\n",
    "    )\n",
    "    frames_student = env.render(\n",
    "        traj_student,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        modify_scene_fns=mod_fns_student,\n",
    "    )\n",
    "\n",
    "    teacher_gif_path = \"teacher_policy.gif\"\n",
    "    student_gif_path = \"student_policy.gif\"\n",
    "    # Ensure frames are uint8\n",
    "    frames_teacher = [np.asarray(f, dtype=np.uint8) for f in frames_teacher]\n",
    "    frames_student = [np.asarray(f, dtype=np.uint8) for f in frames_student]\n",
    "    imageio.mimsave(teacher_gif_path, frames_teacher, fps=fps)\n",
    "    imageio.mimsave(student_gif_path, frames_student, fps=fps)\n",
    "\n",
    "    def gif_to_base64(gif_path):\n",
    "        with open(gif_path, \"rb\") as f:\n",
    "            encoded = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "        return f\"data:image/gif;base64,{encoded}\"\n",
    "\n",
    "    teacher_base64 = gif_to_base64(teacher_gif_path)\n",
    "    student_base64 = gif_to_base64(student_gif_path)\n",
    "    html = f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: center;\">\n",
    "        <div style=\"margin-right: 10px; text-align: center;\">\n",
    "            <h3>Teacher Policy</h3>\n",
    "            <img src=\"{teacher_base64}\" width=\"{width}\" height=\"{height}\"/>\n",
    "        </div>\n",
    "        <div style=\"text-align: center;\">\n",
    "            <h3>Student Policy</h3>\n",
    "            <img src=\"{student_base64}\" width=\"{width}\" height=\"{height}\"/>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "    os.remove(teacher_gif_path)\n",
    "    os.remove(student_gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get teacher logits\n",
    "@jax.jit\n",
    "def get_teacher_logits(observations):\n",
    "    # observations should be the full observation pytree\n",
    "    param_subset = (teacher_params[0], teacher_params[1])\n",
    "    return ppo_network.policy_network.apply(*param_subset, observations)\n",
    "\n",
    "# Training function with MSE loss\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, inputs, targets):\n",
    "    def loss_fn(params):\n",
    "        predictions = student_net.apply(params, inputs)\n",
    "        loss = jnp.mean((predictions - targets) ** 2)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "training_losses = []\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "for episode in range(episodes):\n",
    "    print(f\"\\nEpisode {episode + 1}/{episodes}\")\n",
    "\n",
    "    key, env_key, act_key = jax.random.split(key, 3)\n",
    "    state = jit_reset(env_key)\n",
    "\n",
    "    raw_command = jax.random.uniform(act_key, shape=(3,), minval=0.0, maxval=1.0)\n",
    "    command = jnp.array([\n",
    "        raw_command[0] * env_cfg.command_config.a[0],\n",
    "        raw_command[1] * env_cfg.command_config.a[1],\n",
    "        raw_command[2] * env_cfg.command_config.a[2]\n",
    "    ])\n",
    "\n",
    "    state.info[\"command\"] = command\n",
    "    student_inputs = jnp.zeros((episode_length, student_obs_dim))\n",
    "    student_targets = jnp.zeros((episode_length, 2 * action_size))\n",
    "    rollout = []\n",
    "    modify_scene_fns = []\n",
    "    \n",
    "    for step in range(episode_length):\n",
    "        act_rng, act_key = jax.random.split(act_key)\n",
    "        # Teacher deterministic action using full observation tree\n",
    "        ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "        # Teacher logits target using full observation tree\n",
    "        logits = get_teacher_logits(state.obs)\n",
    "        # Use non-privileged observations as student input\n",
    "        state_flat = state.obs['state']  # shape (student_obs_dim,)\n",
    "        student_inputs = student_inputs.at[step].set(state_flat)\n",
    "        student_targets = student_targets.at[step].set(logits)\n",
    "        state = jit_step(state, ctrl)\n",
    "        state.info[\"command\"] = command\n",
    "        rollout.append(state)\n",
    "        xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "        xyz += np.array([0, 0, 0.2])\n",
    "        x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "        yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "        modify_scene_fns.append(\n",
    "            functools.partial(\n",
    "                draw_joystick_command,\n",
    "                cmd=state.info[\"command\"],\n",
    "                xyz=xyz,\n",
    "                theta=yaw,\n",
    "                scl=abs(state.info[\"command\"][0]) / env_cfg.command_config.a[0],\n",
    "            )\n",
    "        )\n",
    "    total_loss = 0.0\n",
    "    for batch_idx in range(batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_inputs = student_inputs[start_idx:end_idx]\n",
    "        batch_targets = student_targets[start_idx:end_idx]\n",
    "        student_params, opt_state, loss = train_step(student_params, opt_state, batch_inputs, batch_targets)\n",
    "        total_loss += loss\n",
    "    avg_loss = total_loss / batches\n",
    "    training_losses.append(avg_loss)\n",
    "    print(f\"Training Loss: {float(avg_loss):.6f}\")\n",
    "    # Student inference function: outputs logits\n",
    "    student_policy_fn = jax.jit(lambda obs, rng: student_net.apply(student_params, obs))\n",
    "    # Evaluate: convert logits to actions via tanh(mean)\n",
    "    eval_reward = evaluate_policy(env, student_policy_fn, act_key)\n",
    "    print(f\"Student Eval Reward: {eval_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_teacher_student_gifs(\n",
    "    env=env,\n",
    "    jit_reset=jit_reset,\n",
    "    jit_step=jit_step,\n",
    "    teacher_policy_fn=jit_inference_fn,\n",
    "    student_policy_fn=student_policy_fn,\n",
    "    student_params=student_params,\n",
    "    episode_length=episode_length,\n",
    "    command=jnp.array([0.5 * env_cfg.command_config.a[0], 0.0, 0.0]),\n",
    "    seed=seed + episodes,  # use a stable seed after training\n",
    "    width=640,\n",
    "    height=480,\n",
    "    fps=int(1.0 / env.dt / 2),\n",
    "    render_every=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
