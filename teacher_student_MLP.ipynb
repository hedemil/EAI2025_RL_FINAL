{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX Devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Training and visualization script for Go1 with height scanner, including student distillation.\"\"\"\n",
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import functools\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from brax.training.acme import running_statistics\n",
    "import mujoco\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from custom_env import Joystick, default_config\n",
    "from mujoco_playground._src.gait import draw_joystick_command\n",
    "from IPython.display import HTML, display\n",
    "import mediapy as media\n",
    "import imageio\n",
    "import base64\n",
    "\n",
    "# Set environment variables for GPU usage\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "# Verify GPU usage\n",
    "print(\"JAX Devices:\", jax.devices())\n",
    "\n",
    "# Environment setup\n",
    "xml_path = 'custom_env.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "\n",
    "\n",
    "env_cfg = default_config()\n",
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [0.0, 0.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2 * jnp.pi]\n",
    "\n",
    "# Training configuration\n",
    "seed = 42\n",
    "num_envs = 1  # Single environment for simplicity\n",
    "episode_length = 128\n",
    "action_repeat = 1\n",
    "episodes = 100\n",
    "batch_size = 32  # Adjusted for multiple batches\n",
    "batches = episode_length // batch_size\n",
    "learning_rate = 1e-5\n",
    "obs_shape = (52,)  # Matches student_obs_dim\n",
    "action_size = env.action_size\n",
    "student_obs_dim = 52  # From state.obs['state']\n",
    "\n",
    "# Teacher network setup\n",
    "loaded_params = np.load(\"params.npy\", allow_pickle=True)\n",
    "normalizer_params = loaded_params[0]\n",
    "policy_params = loaded_params[1]\n",
    "value_params = loaded_params[2]\n",
    "teacher_params = (normalizer_params, policy_params, value_params)\n",
    "normalize = running_statistics.normalize\n",
    "ppo_params = locomotion_params.brax_ppo_config('Go1JoystickRoughTerrain')\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "if \"network_factory\" in ppo_params:\n",
    "    network_factory = functools.partial(ppo_networks.make_ppo_networks, **ppo_params.network_factory)\n",
    "ppo_network = network_factory(obs_shape, action_size, preprocess_observations_fn=normalize)\n",
    "make_policy = ppo_networks.make_inference_fn(ppo_network)\n",
    "# Teacher inference expects flat state vector (52,)\n",
    "jit_inference_fn = jax.jit(make_policy(teacher_params, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 17:58:30.836145: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-10-02 17:58:31.740986: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-10-02 17:58:32.560608: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student network initialized! Input shape: (32, 52), Output shape: (32, 24)\n"
     ]
    }
   ],
   "source": [
    "# Student network definition\n",
    "class StudentPolicy(nn.Module):\n",
    "    action_size: int\n",
    "    hidden_size: int = 100\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        logits = nn.Dense(features=2 * self.action_size)(x)  # Means and log_stds\n",
    "        return logits\n",
    "\n",
    "# Initialize student network\n",
    "student_net = StudentPolicy(action_size=action_size)\n",
    "\n",
    "dummy_input = jnp.ones((batch_size, student_obs_dim))\n",
    "key_student = jax.random.PRNGKey(42)\n",
    "student_params = student_net.init(key_student, dummy_input)\n",
    "optimizer = optax.adamw(learning_rate)\n",
    "opt_state = optimizer.init(student_params)\n",
    "print(f\"Student network initialized! Input shape: {dummy_input.shape}, Output shape: {(batch_size, 2 * action_size)}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_policy(env, policy_fn, key, steps=episode_length):\n",
    "    state = jit_reset(key)\n",
    "    total_reward = 0.0\n",
    "    for _ in range(steps):\n",
    "        key, act_key = jax.random.split(key)\n",
    "        obs = state.obs['state']\n",
    "        # Add batch dimension for the network\n",
    "        obs_batch = obs.reshape(1, -1)\n",
    "        # Normalize with teacher's normalizer to mirror teacher preprocessing\n",
    "        obs_batch = running_statistics.normalize(normalizer_params, obs_batch)\n",
    "        logits = policy_fn(obs_batch, act_key)\n",
    "        # Convert mean logits to actions via tanh (teacher deterministic action semantics)\n",
    "        mu = logits[0, :action_size]\n",
    "        actions = jnp.tanh(mu)\n",
    "        state = jit_step(state, actions)\n",
    "        total_reward += state.reward\n",
    "    return float(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison visualization function\n",
    "def compare_teacher_student_gifs(\n",
    "    env,\n",
    "    jit_reset,\n",
    "    jit_step,\n",
    "    teacher_policy_fn,\n",
    "    student_policy_fn,\n",
    "    student_params,\n",
    "    episode_length,\n",
    "    command,\n",
    "    seed,\n",
    "    width=640,\n",
    "    height=480,\n",
    "    fps=30,\n",
    "    render_every=2,\n",
    "):\n",
    "    scene_option = mujoco.MjvOption()\n",
    "    scene_option.geomgroup[2] = True\n",
    "    scene_option.geomgroup[3] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key_teacher, key_student, key_env = jax.random.split(key, 3)\n",
    "\n",
    "    state_teacher = jit_reset(key_env)\n",
    "    state_student = jit_reset(key_env)\n",
    "    state_teacher.info[\"command\"] = command\n",
    "    state_student.info[\"command\"] = command\n",
    "\n",
    "    rollout_teacher = []\n",
    "    rollout_student = []\n",
    "    modify_scene_fns_teacher = []\n",
    "    modify_scene_fns_student = []\n",
    "\n",
    "    for step in range(episode_length):\n",
    "        # Teacher\n",
    "        act_rng_teacher, key_teacher = jax.random.split(key_teacher)\n",
    "        # Pass only 'state' observation to teacher inference (matches obs_shape=(52,))\n",
    "        ctrl_teacher, _ = teacher_policy_fn(state_teacher.obs['state'], act_rng_teacher)\n",
    "        state_teacher = jit_step(state_teacher, ctrl_teacher)\n",
    "        state_teacher.info[\"command\"] = command\n",
    "        rollout_teacher.append(state_teacher)\n",
    "\n",
    "        # Student\n",
    "        act_rng_student, key_student = jax.random.split(key_student)\n",
    "        student_obs = state_student.obs['state'].reshape(1, -1)\n",
    "        # Normalize student inputs with teacher's normalizer for consistency\n",
    "        student_obs = running_statistics.normalize(normalizer_params, student_obs)\n",
    "        student_logits = student_policy_fn(student_obs, act_rng_student)\n",
    "        mu = student_logits[0, :env.action_size]\n",
    "        ctrl_student = jnp.tanh(mu)\n",
    "        state_student = jit_step(state_student, ctrl_student)\n",
    "        state_student.info[\"command\"] = command\n",
    "        rollout_student.append(state_student)\n",
    "\n",
    "        for state, modify_scene_fns in [\n",
    "            (state_teacher, modify_scene_fns_teacher),\n",
    "            (state_student, modify_scene_fns_student),\n",
    "        ]:\n",
    "            xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "            xyz += np.array([0, 0, 0.2])\n",
    "            x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "            yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "            modify_scene_fns.append(\n",
    "                functools.partial(\n",
    "                    draw_joystick_command,\n",
    "                    cmd=state.info[\"command\"],\n",
    "                    xyz=xyz,\n",
    "                    theta=yaw,\n",
    "                    scl=abs(state.info[\"command\"][0]) / env_cfg.command_config.a[0],\n",
    "                )\n",
    "            )\n",
    "\n",
    "    traj_teacher = rollout_teacher[::render_every]\n",
    "    traj_student = rollout_student[::render_every]\n",
    "    mod_fns_teacher = modify_scene_fns_teacher[::render_every]\n",
    "    mod_fns_student = modify_scene_fns_student[::render_every]\n",
    "\n",
    "    frames_teacher = env.render(\n",
    "        traj_teacher,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        modify_scene_fns=mod_fns_teacher,\n",
    "    )\n",
    "    frames_student = env.render(\n",
    "        traj_student,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        modify_scene_fns=mod_fns_student,\n",
    "    )\n",
    "\n",
    "    teacher_gif_path = \"teacher_policy.gif\"\n",
    "    student_gif_path = \"student_policy.gif\"\n",
    "    media.write_video(frames_teacher, teacher_gif_path, fps=fps)\n",
    "    media.write_video(frames_student, student_gif_path, fps=fps)\n",
    "\n",
    "    def gif_to_base64(gif_path):\n",
    "        with open(gif_path, \"rb\") as f:\n",
    "            encoded = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "        return f\"data:image/gif;base64,{encoded}\"\n",
    "\n",
    "    teacher_base64 = gif_to_base64(teacher_gif_path)\n",
    "    student_base64 = gif_to_base64(student_gif_path)\n",
    "    html = f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: center;\">\n",
    "        <div style=\"margin-right: 10px; text-align: center;\">\n",
    "            <h3>Teacher Policy</h3>\n",
    "            <img src=\"{teacher_base64}\" width=\"{width}\" height=\"{height}\"/>\n",
    "        </div>\n",
    "        <div style=\"text-align: center;\">\n",
    "            <h3>Student Policy</h3>\n",
    "            <img src=\"{student_base64}\" width=\"{width}\" height=\"{height}\"/>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "    os.remove(teacher_gif_path)\n",
    "    os.remove(student_gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 17:58:37.908916: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n"
     ]
    }
   ],
   "source": [
    "# Function to get teacher logits\n",
    "@jax.jit\n",
    "def get_teacher_logits(observations):\n",
    "    param_subset = (teacher_params[0], teacher_params[1])\n",
    "    return ppo_network.policy_network.apply(*param_subset, observations)\n",
    "\n",
    "# Training function with MSE loss\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, inputs, targets):\n",
    "    def loss_fn(params):\n",
    "        predictions = student_net.apply(params, inputs)\n",
    "        loss = jnp.mean((predictions - targets) ** 2)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "training_losses = []\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "for episode in range(episodes):\n",
    "    print(f\"\\nEpisode {episode + 1}/{episodes}\")\n",
    "\n",
    "    key, env_key, act_key = jax.random.split(key, 3)\n",
    "    state = jit_reset(env_key)\n",
    "\n",
    "    raw_command = jax.random.uniform(act_key, shape=(3,), minval=0.0, maxval=1.0)\n",
    "    command = jnp.array([\n",
    "        raw_command[0] * env_cfg.command_config.a[0],\n",
    "        raw_command[1] * env_cfg.command_config.a[1],\n",
    "        raw_command[2] * env_cfg.command_config.a[2]\n",
    "    ])\n",
    "\n",
    "    state.info[\"command\"] = command\n",
    "    student_inputs = jnp.zeros((episode_length, student_obs_dim))\n",
    "    student_targets = jnp.zeros((episode_length, 2 * action_size))\n",
    "    rollout = []\n",
    "    modify_scene_fns = []\n",
    "    \n",
    "    for step in range(episode_length):\n",
    "        act_rng, act_key = jax.random.split(act_key)\n",
    "        # Teacher deterministic action using 'state' only\n",
    "        ctrl, _ = jit_inference_fn(state.obs['state'], act_rng)\n",
    "        # Teacher logits target using 'state' only to match policy net input\n",
    "        logits = get_teacher_logits(state.obs['state'])\n",
    "        # Normalize student inputs like teacher\n",
    "        state_flat = state.obs['state']\n",
    "        state_flat_norm = running_statistics.normalize(normalizer_params, state_flat)\n",
    "        student_inputs = student_inputs.at[step].set(state_flat_norm)\n",
    "        student_targets = student_targets.at[step].set(logits)\n",
    "        state = jit_step(state, ctrl)\n",
    "        state.info[\"command\"] = command\n",
    "        rollout.append(state)\n",
    "        xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "        xyz += np.array([0, 0, 0.2])\n",
    "        x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "        yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "        modify_scene_fns.append(\n",
    "            functools.partial(\n",
    "                draw_joystick_command,\n",
    "                cmd=state.info[\"command\"],\n",
    "                xyz=xyz,\n",
    "                theta=yaw,\n",
    "                scl=abs(state.info[\"command\"][0]) / env_cfg.command_config.a[0],\n",
    "            )\n",
    "        )\n",
    "    total_loss = 0.0\n",
    "    for batch_idx in range(batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_inputs = student_inputs[start_idx:end_idx]\n",
    "        batch_targets = student_targets[start_idx:end_idx]\n",
    "        student_params, opt_state, loss = train_step(student_params, opt_state, batch_inputs, batch_targets)\n",
    "        total_loss += loss\n",
    "    avg_loss = total_loss / batches\n",
    "    training_losses.append(avg_loss)\n",
    "    print(f\"Training Loss: {avg_loss}\")\n",
    "    # Student inference function: expects normalized input\n",
    "    student_policy_fn = jax.jit(lambda obs, rng: student_net.apply(student_params, obs))\n",
    "    # Evaluate with normalization + tanh(mean)\n",
    "    eval_reward = evaluate_policy(env, student_policy_fn, act_key)\n",
    "    print(f\"Student Eval Reward: {eval_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_teacher_student_gifs(\n",
    "    env=env,\n",
    "    jit_reset=jit_reset,\n",
    "    jit_step=jit_step,\n",
    "    teacher_policy_fn=jit_inference_fn,\n",
    "    student_policy_fn=student_policy_fn,\n",
    "    student_params=student_params,\n",
    "    episode_length=episode_length,\n",
    "    command=command,\n",
    "    seed=seed + episode,\n",
    "    width=640,\n",
    "    height=480,\n",
    "    fps=int(1.0 / env.dt / 2),\n",
    "    render_every=2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
