{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher-Student Distillation\n",
    "\n",
    "In the following code we're loading a policy trained from train.ipynb that has access to privileged information and we train a student network that relies solely on the non-privileged state to mimic the behaviour of the teacher.\n",
    "\n",
    "Our approach is an on-policy student learning that steps the environment based on the student actions and is trained by computing how the teacher would behave in the same situation, similar to DAgger (https://imitation.readthedocs.io/en/latest/algorithms/dagger.html, https://arxiv.org/abs/1011.0686)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training and visualization script for Go1 with height scanner, including student distillation.\"\"\"\n",
    "import os\n",
    "\n",
    "# Set environment variables BEFORE importing mujoco to ensure headless offscreen rendering\n",
    "xla_flags = os.environ.get('XLA_FLAGS', '')\n",
    "xla_flags += ' --xla_gpu_triton_gemm_any=True'\n",
    "os.environ['XLA_FLAGS'] = xla_flags\n",
    "os.environ['MUJOCO_GL'] = 'egl'\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import functools\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from brax.training.acme import running_statistics\n",
    "import mujoco\n",
    "from mujoco_playground import wrapper\n",
    "from mujoco_playground.config import locomotion_params\n",
    "from custom_env import Joystick, default_config\n",
    "from mujoco_playground._src.gait import draw_joystick_command\n",
    "from IPython.display import HTML, display, clear_output\n",
    "import mediapy as media\n",
    "import imageio\n",
    "import base64\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Environment setup\n",
    "xml_path = 'custom_env.xml'\n",
    "env = Joystick(xml_path=xml_path, config=default_config())\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "# Batched env helpers (parallelize over environments)\n",
    "batched_reset = jax.jit(jax.vmap(env.reset))\n",
    "batched_step = jax.jit(jax.vmap(env.step))\n",
    "\n",
    "env_cfg = default_config()\n",
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [0.0, 0.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2 * jnp.pi]\n",
    "\n",
    "# Training configuration\n",
    "seed = 42\n",
    "num_envs = 128  # Parallel environments for batched rollout\n",
    "episode_length = 400\n",
    "action_repeat = 1\n",
    "episodes = 800\n",
    "batch_size = 64\n",
    "batches = (episode_length * num_envs) // batch_size\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Discover observation/action shapes from the real env\n",
    "_dummy_key = jax.random.PRNGKey(0)\n",
    "dummy_state = jit_reset(_dummy_key)\n",
    "obs_shape = jax.tree_util.tree_map(lambda x: x.shape, dummy_state.obs)\n",
    "action_size = env.action_size\n",
    "student_obs_dim = int(dummy_state.obs['state'].shape[0])\n",
    "\n",
    "# Teacher network setup (load params saved by train.ipynb)\n",
    "_loaded = np.load(\"params.npy\", allow_pickle=True)\n",
    "# np.save of a tuple can load either as 0-d object array or (3,) object array\n",
    "if getattr(_loaded, 'ndim', 1) == 0:\n",
    "    normalizer_params, policy_params, value_params = _loaded.item()\n",
    "else:\n",
    "    normalizer_params, policy_params, value_params = tuple(_loaded.tolist())\n",
    "teacher_params = (normalizer_params, policy_params, value_params)\n",
    "\n",
    "normalize = running_statistics.normalize\n",
    "ppo_params = locomotion_params.brax_ppo_config('Go1JoystickRoughTerrain')\n",
    "network_factory = ppo_networks.make_ppo_networks\n",
    "if hasattr(ppo_params, 'network_factory'):\n",
    "    network_factory = functools.partial(ppo_networks.make_ppo_networks, **ppo_params.network_factory)\n",
    "\n",
    "# Build teacher network with the SAME observation structure used at training time\n",
    "ppo_network = network_factory(obs_shape, action_size, preprocess_observations_fn=normalize)\n",
    "make_policy = ppo_networks.make_inference_fn(ppo_network)\n",
    "# Teacher inference expects full observation tree (state.obs)\n",
    "jit_inference_fn = jax.jit(make_policy(teacher_params, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student network definition\n",
    "class StudentPolicy(nn.Module):\n",
    "    action_size: int\n",
    "    hidden_size: int = 256\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_size)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=(self.hidden_size//2))(x)\n",
    "        x = nn.relu(x)\n",
    "        logits = nn.Dense(features=2 * self.action_size)(x)  # Means and log_stds\n",
    "        return logits\n",
    "\n",
    "# Initialize student network\n",
    "student_net = StudentPolicy(action_size=action_size)\n",
    "\n",
    "dummy_input = jnp.ones((batch_size, student_obs_dim))\n",
    "key_student = jax.random.PRNGKey(42)\n",
    "student_params = student_net.init(key_student, dummy_input)\n",
    "optimizer = optax.adamw(learning_rate)\n",
    "opt_state = optimizer.init(student_params)\n",
    "print(f\"Student network initialized! Input shape: {dummy_input.shape}, Output shape: {(batch_size, 2 * action_size)}\")\n",
    "\n",
    "# Vectorized apply for student (maps across leading batch dim)\n",
    "apply_student_batched = jax.jit(jax.vmap(lambda p, x: student_net.apply(p, x)))\n",
    "\n",
    "# Evaluation function\n",
    "# policy_fn should be a function: (obs_batch: (B, student_obs_dim), rng) -> logits (B, 2*action_size)\n",
    "def evaluate_policy(env, policy_fn, key, steps=episode_length, B=1):\n",
    "    # Reset B envs; default B=1 for eval\n",
    "    keys = jax.random.split(key, B)\n",
    "    states = jax.vmap(jit_reset)(keys) if B > 1 else jit_reset(key)\n",
    "    total_reward = 0.0\n",
    "    for _ in range(steps):\n",
    "        if B > 1:\n",
    "            key_step = jax.random.split(key, 1)[0]\n",
    "            obs = states.obs['state']  # (B, student_obs_dim)\n",
    "            logits = policy_fn(obs, key_step)\n",
    "            mu = logits[:, :action_size]\n",
    "            actions = jnp.tanh(mu)\n",
    "            states = jax.vmap(jit_step)(states, actions)\n",
    "            total_reward += jnp.sum(states.reward)\n",
    "        else:\n",
    "            key, act_key = jax.random.split(key)\n",
    "            obs = states.obs['state'].reshape(1, -1)\n",
    "            logits = policy_fn(obs, act_key)\n",
    "            mu = logits[0, :action_size]\n",
    "            actions = jnp.tanh(mu)\n",
    "            states = jit_step(states, actions)\n",
    "            total_reward += states.reward\n",
    "    return float(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison visualization function\n",
    "def compare_teacher_student_gifs(\n",
    "    env,\n",
    "    jit_reset,\n",
    "    jit_step,\n",
    "    teacher_policy_fn,\n",
    "    student_policy_fn,\n",
    "    student_params,\n",
    "    episode_length,\n",
    "    command,\n",
    "    seed,\n",
    "    width=640,\n",
    "    height=480,\n",
    "    fps=30,\n",
    "    render_every=2,\n",
    "):\n",
    "    global video_handle\n",
    "    scene_option = mujoco.MjvOption()\n",
    "    scene_option.geomgroup[2] = True\n",
    "    scene_option.geomgroup[3] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    key_teacher, key_student, key_env = jax.random.split(key, 3)\n",
    "    state_teacher = jit_reset(key_env)\n",
    "    state_student = jit_reset(key_env)\n",
    "    state_teacher.info[\"command\"] = command\n",
    "    state_student.info[\"command\"] = command\n",
    "    rollout_teacher = []\n",
    "    rollout_student = []\n",
    "    modify_scene_fns_teacher = []\n",
    "    modify_scene_fns_student = []\n",
    "    for _ in range(episode_length):\n",
    "        # Teacher: expects full observation tree\n",
    "        act_rng_teacher, key_teacher = jax.random.split(key_teacher)\n",
    "        ctrl_teacher, _ = teacher_policy_fn(state_teacher.obs, act_rng_teacher)\n",
    "        state_teacher = jit_step(state_teacher, ctrl_teacher)\n",
    "        state_teacher.info[\"command\"] = command\n",
    "        rollout_teacher.append(state_teacher)\n",
    "        # Student: logits -> tanh(mu) actions\n",
    "        act_rng_student, key_student = jax.random.split(key_student)\n",
    "        student_obs = state_student.obs['state'].reshape(1, -1)\n",
    "        student_logits = student_policy_fn(student_obs, act_rng_student)\n",
    "        mu = student_logits[0, :env.action_size]\n",
    "        ctrl_student = jnp.tanh(mu)\n",
    "        state_student = jit_step(state_student, ctrl_student)\n",
    "        state_student.info[\"command\"] = command\n",
    "        rollout_student.append(state_student)\n",
    "        for st, mod_list in [\n",
    "            (state_teacher, modify_scene_fns_teacher),\n",
    "            (state_student, modify_scene_fns_student),\n",
    "        ]:\n",
    "            xyz = np.array(st.data.xpos[env._torso_body_id])\n",
    "            xyz += np.array([0, 0, 0.2])\n",
    "            x_axis = st.data.xmat[env._torso_body_id, 0]\n",
    "            yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "            mod_list.append(\n",
    "                functools.partial(\n",
    "                    draw_joystick_command,\n",
    "                    cmd=st.info[\"command\"],\n",
    "                    xyz=xyz,\n",
    "                    theta=yaw,\n",
    "                    scl=abs(st.info[\"command\"][0]) / env_cfg.command_config.a[0],\n",
    "                )\n",
    "            )\n",
    "    traj_teacher = rollout_teacher[::render_every]\n",
    "    traj_student = rollout_student[::render_every]\n",
    "    mod_fns_teacher = modify_scene_fns_teacher[::render_every]\n",
    "    mod_fns_student = modify_scene_fns_student[::render_every]\n",
    "    # Use env.render which is implemented to render offscreen with EGL\n",
    "    frames_teacher = env.render(\n",
    "        traj_teacher,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        modify_scene_fns=mod_fns_teacher,\n",
    "    )\n",
    "    frames_student = env.render(\n",
    "        traj_student,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        modify_scene_fns=mod_fns_student,\n",
    "    )\n",
    "    teacher_gif_path = \"teacher_policy.gif\"\n",
    "    student_gif_path = \"student_policy.gif\"\n",
    "    # Ensure frames are uint8\n",
    "    frames_teacher = [np.asarray(f, dtype=np.uint8) for f in frames_teacher]\n",
    "    frames_student = [np.asarray(f, dtype=np.uint8) for f in frames_student]\n",
    "    imageio.mimsave(teacher_gif_path, frames_teacher, fps=fps, loop=0)\n",
    "    imageio.mimsave(student_gif_path, frames_student, fps=fps, loop=0)\n",
    "    def gif_to_base64(gif_path):\n",
    "        with open(gif_path, \"rb\") as f:\n",
    "            encoded = base64.b64encode(f.read()).decode(\"ascii\")\n",
    "        return f\"data:image/gif;base64,{encoded}\"\n",
    "    teacher_base64 = gif_to_base64(teacher_gif_path)\n",
    "    student_base64 = gif_to_base64(student_gif_path)\n",
    "    html = f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: center;\">\n",
    "        <div style=\"margin-right: 10px; text-align: center;\">\n",
    "            <h3>Teacher Policy</h3>\n",
    "            <img src=\"{teacher_base64}\" width=\"{width}\" height=\"{height}\"/>\n",
    "        </div>\n",
    "        <div style=\"text-align: center;\">\n",
    "            <h3>Student Policy</h3>\n",
    "            <img src=\"{student_base64}\" width=\"{width}\" height=\"{height}\"/>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    \n",
    "    if video_handle is None:\n",
    "        video_handle = display(HTML(html), display_id=True)\n",
    "    else:\n",
    "        video_handle.update(HTML(html))\n",
    "\n",
    "    os.remove(teacher_gif_path)\n",
    "    os.remove(student_gif_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get teacher logits\n",
    "@jax.jit\n",
    "def get_teacher_logits(observations):\n",
    "    # observations can be single or batched observation pytree\n",
    "    param_subset = (teacher_params[0], teacher_params[1])\n",
    "    return ppo_network.policy_network.apply(*param_subset, observations)\n",
    "\n",
    "# Training function with MSE loss\n",
    "@jax.jit\n",
    "def train_step(params, opt_state, inputs, targets):\n",
    "    def loss_fn(params):\n",
    "        predictions = student_net.apply(params, inputs)\n",
    "        loss = jnp.mean((predictions - targets) ** 2)\n",
    "        return loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "training_losses = []\n",
    "eval_rewards = []\n",
    "eval_episodes = []\n",
    "metrics_handle = None\n",
    "video_handle = None\n",
    "\n",
    "key = jax.random.PRNGKey(seed)\n",
    "for episode in range(episodes):\n",
    "    key, env_key, act_key = jax.random.split(key, 3)\n",
    "    # Reset num_envs in parallel\n",
    "    env_keys = jax.random.split(env_key, num_envs)\n",
    "    states = batched_reset(env_keys)\n",
    "\n",
    "    # Generate a different random command for each environment: x~U(0,1), y~U(0,1), z=0\n",
    "    act_key, cmd_key = jax.random.split(act_key)\n",
    "    rand_xyz = jax.random.uniform(cmd_key, shape=(num_envs, 3), minval=0.0, maxval=1.0)\n",
    "    # zeros_z = jnp.zeros((num_envs, 1))\n",
    "    # command_batch = jnp.concatenate([rand_xy, zeros_z], axis=-1)  # (N, 3)\n",
    "    command_batch = rand_xyz\n",
    "    states.info[\"command\"] = command_batch\n",
    "\n",
    "    # Buffers: (T, N, ...)\n",
    "    student_inputs = jnp.zeros((episode_length, num_envs, student_obs_dim))\n",
    "    student_targets = jnp.zeros((episode_length, num_envs, 2 * action_size))\n",
    "\n",
    "    for step in range(episode_length):\n",
    "        # Teacher logits target on current states (batched)\n",
    "        logits = get_teacher_logits(states.obs)  # (N, 2*A)\n",
    "\n",
    "        obs_batch = states.obs['state']  # (N, student_obs_dim)\n",
    "        student_inputs = student_inputs.at[step].set(obs_batch)\n",
    "        student_targets = student_targets.at[step].set(logits)\n",
    "\n",
    "        # Student action: tanh(mean)\n",
    "        student_logits = student_net.apply(student_params, obs_batch)  # (N, 2*A)\n",
    "        mu = student_logits[:, :action_size]\n",
    "        actions = jnp.tanh(mu)  # (N, A)\n",
    "        states = batched_step(states, actions)\n",
    "        states.info[\"command\"] = command_batch\n",
    "\n",
    "    # Flatten buffers to (T*N, ...)\n",
    "    flat_inputs = student_inputs.reshape((-1, student_obs_dim))\n",
    "    flat_targets = student_targets.reshape((-1, 2 * action_size))\n",
    "\n",
    "    # Mini-batch SGD over collected data; use only full-sized minibatches to avoid recompilation\n",
    "    total_samples = flat_inputs.shape[0]\n",
    "    total_full = (total_samples // batch_size) * batch_size\n",
    "    num_minibatches = max(1, total_full // batch_size)\n",
    "    total_loss = 0.0\n",
    "    for b in range(num_minibatches):\n",
    "        start_idx = b * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_in = flat_inputs[start_idx:end_idx]\n",
    "        batch_tg = flat_targets[start_idx:end_idx]\n",
    "        student_params, opt_state, loss = train_step(student_params, opt_state, batch_in, batch_tg)\n",
    "        total_loss += loss\n",
    "    avg_loss = total_loss / num_minibatches\n",
    "    training_losses.append(float(avg_loss))\n",
    "\n",
    "    if (episode + 1) % 30 == 0:\n",
    "        # Student inference function: outputs logits for a batch; eval with B=1 for simplicity\n",
    "        student_policy_fn = jax.jit(lambda obs, rng: student_net.apply(student_params, obs))\n",
    "        eval_reward = evaluate_policy(env, student_policy_fn, act_key, steps=episode_length, B=1)\n",
    "        eval_rewards.append(float(eval_reward))\n",
    "        eval_episodes.append(episode + 1)\n",
    "\n",
    "        global metrics_handle\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        axes[0].plot(range(1, len(training_losses) + 1), [float(x) for x in training_losses], label=\"Training Loss\")\n",
    "        axes[0].set_xlabel(\"Episode\")\n",
    "        axes[0].set_ylabel(\"MSE Loss\")\n",
    "        axes[0].legend()\n",
    "        axes[0].set_title(\"Training Loss\")\n",
    "        axes[1].plot(eval_episodes, eval_rewards, label=\"Eval Reward\")\n",
    "        axes[1].set_xlabel(\"Episode\")\n",
    "        axes[1].set_ylabel(\"Eval Reward\")\n",
    "        axes[1].legend()\n",
    "        axes[1].set_title(\"Evaluation Reward\")\n",
    "        plt.tight_layout()\n",
    "        if metrics_handle is None:\n",
    "            metrics_handle = display(fig, display_id=True)\n",
    "        else:\n",
    "            metrics_handle.update(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "    # Render teacher vs student GIFs every 100 episodes\n",
    "    if (episode + 1) % 200 == 0:\n",
    "        env_cfg.pert_config.enable = True\n",
    "        env_cfg.pert_config.velocity_kick = [3.0, 6.0]\n",
    "        env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "        env_cfg.command_config.a = [1.5, 0.8, 2*jnp.pi]\n",
    "        eval_env = env\n",
    "        velocity_kick_range = [0.0, 0.0]  # Disable velocity kick.\n",
    "        kick_duration_range = [0.05, 0.2]\n",
    "\n",
    "        jit_reset = jax.jit(eval_env.reset)\n",
    "        jit_step = jax.jit(eval_env.step)\n",
    "        student_policy_fn = jax.jit(lambda obs, rng: student_net.apply(student_params, obs))\n",
    "\n",
    "        x_vels = [0.0, 0.5, 1.0, 0.0, 0.0, 0.5]\n",
    "        y_vels = [0.0, 0.0, 0.0, 0.5, 1.0, 0.5]\n",
    "        yaw_vels = [1.0, 0.0, 0.0, 0.0, 0.0, 0.1]\n",
    "\n",
    "        for run_id, (x_vel, y_vel, yaw_vel) in enumerate(zip(x_vels, y_vels, yaw_vels)):\n",
    "            def sample_pert(rng):\n",
    "                rng, key1, key2 = jax.random.split(rng, 3)\n",
    "                pert_mag = jax.random.uniform(\n",
    "                    key1, minval=velocity_kick_range[0], maxval=velocity_kick_range[1]\n",
    "                )\n",
    "                duration_seconds = jax.random.uniform(\n",
    "                    key2, minval=kick_duration_range[0], maxval=kick_duration_range[1]\n",
    "                )\n",
    "                duration_steps = jnp.round(duration_seconds / eval_env.dt).astype(jnp.int32)\n",
    "                state.info[\"pert_mag\"] = pert_mag\n",
    "                state.info[\"pert_duration\"] = duration_steps\n",
    "                state.info[\"pert_duration_seconds\"] = duration_seconds\n",
    "                return rng\n",
    "\n",
    "\n",
    "            rng = jax.random.PRNGKey(0)\n",
    "            rollout = []\n",
    "            modify_scene_fns = []\n",
    "\n",
    "            swing_peak = []\n",
    "            rewards = []\n",
    "            linvel = []\n",
    "            angvel = []\n",
    "            track = []\n",
    "            foot_vel = []\n",
    "            rews = []\n",
    "            contact = []\n",
    "            command = jnp.array([x_vel, y_vel, yaw_vel])\n",
    "\n",
    "            state = jit_reset(rng)\n",
    "            if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
    "                rng = sample_pert(rng)\n",
    "            state.info[\"command\"] = command\n",
    "            for _ in range(env_cfg.episode_length):\n",
    "                if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
    "                    rng = sample_pert(rng)\n",
    "                act_rng, rng = jax.random.split(rng)\n",
    "                student_obs = state.obs['state'].reshape(1, -1)\n",
    "                student_logits = student_policy_fn(student_obs, rng)\n",
    "                mu = student_logits[0, :env.action_size]\n",
    "                ctrl_student = jnp.tanh(mu)\n",
    "                state = jit_step(state, ctrl_student)\n",
    "                state.info[\"command\"] = command\n",
    "                rews.append(\n",
    "                    {k: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "                )\n",
    "                rollout.append(state)\n",
    "                swing_peak.append(state.info[\"swing_peak\"])\n",
    "                rewards.append(\n",
    "                    {k[7:]: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "                )\n",
    "                linvel.append(env.get_global_linvel(state.data))\n",
    "                angvel.append(env.get_gyro(state.data))\n",
    "                track.append(\n",
    "                    env._reward_tracking_lin_vel(\n",
    "                        state.info[\"command\"], env.get_local_linvel(state.data)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                feet_vel = state.data.sensordata[env._foot_linvel_sensor_adr]\n",
    "                vel_xy = feet_vel[..., :2]\n",
    "                vel_norm = jnp.sqrt(jnp.linalg.norm(vel_xy, axis=-1))\n",
    "                foot_vel.append(vel_norm)\n",
    "\n",
    "                contact.append(state.info[\"last_contact\"])\n",
    "\n",
    "                xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "                xyz += np.array([0, 0, 0.2])\n",
    "                x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "                yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "                modify_scene_fns.append(\n",
    "                    functools.partial(\n",
    "                        draw_joystick_command,\n",
    "                        cmd=state.info[\"command\"],\n",
    "                        xyz=xyz,\n",
    "                        theta=yaw,\n",
    "                        scl=abs(state.info[\"command\"][0])\n",
    "                        / env_cfg.command_config.a[0],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "            render_every = 2\n",
    "            fps = 1.0 / eval_env.dt / render_every\n",
    "            traj = rollout[::render_every]\n",
    "            mod_fns = modify_scene_fns[::render_every]\n",
    "\n",
    "            scene_option = mujoco.MjvOption()\n",
    "            scene_option.geomgroup[2] = True\n",
    "            scene_option.geomgroup[3] = False\n",
    "            scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "            scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "            scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "            frames = eval_env.render(\n",
    "                traj,\n",
    "                camera=\"track\",\n",
    "                scene_option=scene_option,\n",
    "                width=640,\n",
    "                height=480,\n",
    "                modify_scene_fns=mod_fns,\n",
    "            )\n",
    "            media.show_video(frames, fps=fps)\n",
    "\n",
    "            filename = f\"output/{episode+1}/teacher_student_{run_id}.gif\"\n",
    "            imageio.mimsave(filename, frames, duration=1.0 / fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation, from utils.py, changing how we compute the action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_cfg.pert_config.enable = True\n",
    "env_cfg.pert_config.velocity_kick = [3.0, 6.0]\n",
    "env_cfg.pert_config.kick_wait_times = [5.0, 15.0]\n",
    "env_cfg.command_config.a = [1.5, 0.8, 2*jnp.pi]\n",
    "eval_env = env\n",
    "velocity_kick_range = [0.0, 0.0]  # Disable velocity kick.\n",
    "kick_duration_range = [0.05, 0.2]\n",
    "\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "student_policy_fn = jax.jit(lambda obs, rng: student_net.apply(student_params, obs))\n",
    "\n",
    "x_vels = [0.0, 0.5, 1.0, 0.0, 0.0, 0.5]\n",
    "y_vels = [0.0, 0.0, 0.0, 0.5, 1.0, 0.5]\n",
    "yaw_vels = [1.0, 0.0, 0.0, 0.0, 0.0, 0.1]\n",
    "\n",
    "for run_id, (x_vel, y_vel, yaw_vel) in enumerate(zip(x_vels, y_vels, yaw_vels)):\n",
    "    def sample_pert(rng):\n",
    "        rng, key1, key2 = jax.random.split(rng, 3)\n",
    "        pert_mag = jax.random.uniform(\n",
    "            key1, minval=velocity_kick_range[0], maxval=velocity_kick_range[1]\n",
    "        )\n",
    "        duration_seconds = jax.random.uniform(\n",
    "            key2, minval=kick_duration_range[0], maxval=kick_duration_range[1]\n",
    "        )\n",
    "        duration_steps = jnp.round(duration_seconds / eval_env.dt).astype(jnp.int32)\n",
    "        state.info[\"pert_mag\"] = pert_mag\n",
    "        state.info[\"pert_duration\"] = duration_steps\n",
    "        state.info[\"pert_duration_seconds\"] = duration_seconds\n",
    "        return rng\n",
    "\n",
    "\n",
    "    rng = jax.random.PRNGKey(0)\n",
    "    rollout = []\n",
    "    modify_scene_fns = []\n",
    "\n",
    "    swing_peak = []\n",
    "    rewards = []\n",
    "    linvel = []\n",
    "    angvel = []\n",
    "    track = []\n",
    "    foot_vel = []\n",
    "    rews = []\n",
    "    contact = []\n",
    "    command = jnp.array([x_vel, y_vel, yaw_vel])\n",
    "\n",
    "    state = jit_reset(rng)\n",
    "    if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
    "        rng = sample_pert(rng)\n",
    "    state.info[\"command\"] = command\n",
    "    for _ in range(env_cfg.episode_length):\n",
    "        if state.info[\"steps_since_last_pert\"] < state.info[\"steps_until_next_pert\"]:\n",
    "            rng = sample_pert(rng)\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        student_obs = state.obs['state'].reshape(1, -1)\n",
    "        student_logits = student_policy_fn(student_obs, rng)\n",
    "        mu = student_logits[0, :env.action_size]\n",
    "        ctrl_student = jnp.tanh(mu)\n",
    "        state = jit_step(state, ctrl_student)\n",
    "        state.info[\"command\"] = command\n",
    "        rews.append(\n",
    "            {k: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "        )\n",
    "        rollout.append(state)\n",
    "        swing_peak.append(state.info[\"swing_peak\"])\n",
    "        rewards.append(\n",
    "            {k[7:]: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n",
    "        )\n",
    "        linvel.append(env.get_global_linvel(state.data))\n",
    "        angvel.append(env.get_gyro(state.data))\n",
    "        track.append(\n",
    "            env._reward_tracking_lin_vel(\n",
    "                state.info[\"command\"], env.get_local_linvel(state.data)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        feet_vel = state.data.sensordata[env._foot_linvel_sensor_adr]\n",
    "        vel_xy = feet_vel[..., :2]\n",
    "        vel_norm = jnp.sqrt(jnp.linalg.norm(vel_xy, axis=-1))\n",
    "        foot_vel.append(vel_norm)\n",
    "\n",
    "        contact.append(state.info[\"last_contact\"])\n",
    "\n",
    "        xyz = np.array(state.data.xpos[env._torso_body_id])\n",
    "        xyz += np.array([0, 0, 0.2])\n",
    "        x_axis = state.data.xmat[env._torso_body_id, 0]\n",
    "        yaw = -np.arctan2(x_axis[1], x_axis[0])\n",
    "        modify_scene_fns.append(\n",
    "            functools.partial(\n",
    "                draw_joystick_command,\n",
    "                cmd=state.info[\"command\"],\n",
    "                xyz=xyz,\n",
    "                theta=yaw,\n",
    "                scl=abs(state.info[\"command\"][0])\n",
    "                / env_cfg.command_config.a[0],\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    render_every = 2\n",
    "    fps = 1.0 / eval_env.dt / render_every\n",
    "    traj = rollout[::render_every]\n",
    "    mod_fns = modify_scene_fns[::render_every]\n",
    "\n",
    "    scene_option = mujoco.MjvOption()\n",
    "    scene_option.geomgroup[2] = True\n",
    "    scene_option.geomgroup[3] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n",
    "    scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n",
    "\n",
    "    frames = eval_env.render(\n",
    "        traj,\n",
    "        camera=\"track\",\n",
    "        scene_option=scene_option,\n",
    "        width=640,\n",
    "        height=480,\n",
    "        modify_scene_fns=mod_fns,\n",
    "    )\n",
    "    media.show_video(frames, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
